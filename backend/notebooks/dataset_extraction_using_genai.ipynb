{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=>\n",
      "LayoutLM: Pre-training of Text and Layout for\n",
      "Document Image Understanding\n",
      "\n",
      "=>\n",
      "Yiheng Xu∗\n",
      "\n",
      "=>\n",
      "charlesyihengxu@gmail.com\n",
      "Harbin Institute of Technology\n",
      "\n",
      "=>\n",
      "Minghao Li∗\n",
      "\n",
      "=>\n",
      "liminghao1630@buaa.edu.cn\n",
      "Beihang University\n",
      "\n",
      "=>\n",
      "Lei Cui\n",
      "lecu@microsoft.com\n",
      "Microsoft Research Asia\n",
      "\n",
      "=>\n",
      "Shaohan Huang\n",
      "shaohanh@microsoft.com\n",
      "Microsoft Research Asia\n",
      "\n",
      "=>\n",
      "Furu Wei\n",
      "fuwei@microsoft.com\n",
      "Microsoft Research Asia\n",
      "\n",
      "=>\n",
      "Ming Zhou\n",
      "mingzhou@microsoft.com\n",
      "Microsoft Research Asia\n",
      "\n",
      "=>\n",
      "ABSTRACT\n",
      "\n",
      "=>\n",
      "Pre-training techniques have been verified successfully in a vari-\n",
      "ety of NLP tasks in recent years. Despite the widespread use of\n",
      "pre-training models for NLP applications, they almost exclusively\n",
      "focus on text-level manipulation, while neglecting layout and style\n",
      "information that is vital for document image understanding. In\n",
      "this paper, we propose the LayoutLM to jointly model interactions\n",
      "between text and layout information across scanned document\n",
      "images, which is beneficial for a great number of real-world doc-\n",
      "ument image understanding tasks such as information extraction\n",
      "from scanned documents. Furthermore, we also leverage image\n",
      "features to incorporate words’ visual information into LayoutLM.\n",
      "To the best of our knowledge, this is the first time that text and\n",
      "layout are jointly learned in a single framework for document-\n",
      "level pre-training. It achieves new state-of-the-art results in several\n",
      "downstream tasks, including form understanding (from 70.72 to\n",
      "79.27), receipt understanding (from 94.02 to 95.24) and document\n",
      "image classification (from 93.07 to 94.42). The code and pre-trained\n",
      "LayoutLM models are publicly available at https://aka.ms/layoutlm.\n",
      "\n",
      "=>\n",
      "CCS CONCEPTS\n",
      "\n",
      "=>\n",
      "• Information systems →Business intelligence; • Computing\n",
      "methodologies →Information extraction; Transfer learning;\n",
      "• Applied computing →Document analysis.\n",
      "\n",
      "=>\n",
      "KEYWORDS\n",
      "\n",
      "=>\n",
      "LayoutLM; pre-trained models; document image understanding\n",
      "\n",
      "=>\n",
      "ACM Reference Format:\n",
      "Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming\n",
      "Zhou. 2020. LayoutLM: Pre-training of Text and Layout for Document\n",
      "Image Understanding. In Proceedings of the 26th ACM SIGKDD Conference\n",
      "on Knowledge Discovery and Data Mining (KDD ’20), August 23–27, 2020,\n",
      "Virtual Event, CA, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/\n",
      "10.1145/3394486.3403172\n",
      "\n",
      "=>\n",
      "∗Equal contributions during internship at Microsoft Research Asia.\n",
      "\n",
      "=>\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior specific permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "KDD ’20, August 23–27, 2020, Virtual Event, CA, USA\n",
      "© 2020 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-7998-4/20/08...$15.00\n",
      "https://doi.org/10.1145/3394486.3403172\n",
      "\n",
      "=>\n",
      "1\n",
      "INTRODUCTION\n",
      "\n",
      "=>\n",
      "Document AI, or Document Intelligence1, is a relatively new re-\n",
      "search topic that refers techniques for automatically reading, under-\n",
      "standing, and analyzing business documents. Business documents\n",
      "are files that provide details related to a company’s internal and\n",
      "external transactions, which are shown in Figure 1. They may be\n",
      "digital-born, occurring as electronic files, or they may be in scanned\n",
      "form that comes from written or printed on paper. Some common\n",
      "examples of business documents include purchase orders, financial\n",
      "reports, business emails, sales agreements, vendor contracts, letters,\n",
      "invoices, receipts, resumes, and many others. Business documents\n",
      "are critical to a company’s efficiency and productivity. The exact\n",
      "format of a business document may vary, but the information is\n",
      "usually presented in natural language and can be organized in a\n",
      "variety of ways from plain text, multi-column layouts, and a wide\n",
      "variety of tables/forms/figures. Understanding business documents\n",
      "is a very challenging task due to the diversity of layouts and formats,\n",
      "poor quality of scanned document images as well as the complexity\n",
      "of template structures.\n",
      "Nowadays, many companies extract data from business docu-\n",
      "ments through manual efforts that are time-consuming and expen-\n",
      "sive, meanwhile requiring manual customization or configuration.\n",
      "Rules and workflows for each type of document often need to be\n",
      "hard-coded and updated with changes to the specific format or\n",
      "when dealing with multiple formats. To address these problems,\n",
      "document AI models and algorithms are designed to automatically\n",
      "classify, extract, and structuralize information from business doc-\n",
      "uments, accelerating automated document processing workflows.\n",
      "Contemporary approaches for document AI are usually built upon\n",
      "deep neural networks from a computer vision perspective or a natu-\n",
      "ral language processing perspective, or a combination of them. Early\n",
      "attempts usually focused on detecting and analyzing certain parts\n",
      "of a document, such as tabular areas. [7] were the first to propose a\n",
      "table detection method for PDF documents based on Convolutional\n",
      "Neural Networks (CNN). After that, [21, 24, 29] also leveraged more\n",
      "advanced Faster R-CNN model [19] or Mask R-CNN model [9] to\n",
      "further improve the accuracy of document layout analysis. In addi-\n",
      "tion, [28] presented an end-to-end, multimodal, fully convolutional\n",
      "network for extracting semantic structures from document images,\n",
      "taking advantage of text embeddings from pre-trained NLP models.\n",
      "More recently, [15] introduced a Graph Convolutional Networks\n",
      "(GCN) based model to combine textual and visual information for\n",
      "\n",
      "=>\n",
      "1https://sites.google.com/view/di2019\n",
      "\n",
      "=>\n",
      "arXiv:1912.13318v5  [cs.CL]  16 Jun 2020\n",
      "\n",
      "=>\n",
      "(a)\n",
      "(b)\n",
      "(c)\n",
      "(d)\n",
      "\n",
      "=>\n",
      "Figure 1: Scanned images of business documents with different layouts and formats\n",
      "\n",
      "=>\n",
      "information extraction from business documents. Although these\n",
      "models have made significant progress in the document AI area\n",
      "with deep neural networks, most of these methods confront two\n",
      "limitations: (1) They rely on a few human-labeled training samples\n",
      "without fully exploring the possibility of using large-scale unla-\n",
      "beled training samples. (2) They usually leverage either pre-trained\n",
      "CV models or NLP models, but do not consider a joint training of\n",
      "textual and layout information. Therefore, it is important to inves-\n",
      "tigate how self-supervised pre-training of text and layout may help\n",
      "in the document AI area.\n",
      "To this end, we propose LayoutLM, a simple yet effective pre-\n",
      "training method of text and layout for document image understand-\n",
      "ing tasks. Inspired by the BERT model [4], where input textual\n",
      "information is mainly represented by text embeddings and position\n",
      "embeddings, LayoutLM further adds two types of input embeddings:\n",
      "(1) a 2-D position embedding that denotes the relative position of\n",
      "a token within a document; (2) an image embedding for scanned\n",
      "token images within a document. The architecture of LayoutLM is\n",
      "shown in Figure 2. We add these two input embeddings because\n",
      "the 2-D position embedding can capture the relationship among\n",
      "tokens within a document, meanwhile the image embedding can\n",
      "capture some appearance features such as font directions, types,\n",
      "and colors. In addition, we adopt a multi-task learning objective for\n",
      "LayoutLM, including a Masked Visual-Language Model (MVLM)\n",
      "loss and a Multi-label Document Classification (MDC) loss, which\n",
      "further enforces joint pre-training for text and layout. In this work,\n",
      "our focus is the document pre-training based on scanned docu-\n",
      "ment images, while digital-born documents are less challenging\n",
      "because they can be considered as a special case where OCR is\n",
      "not required, thus they are out of the scope of this paper. Specifi-\n",
      "cally, the LayoutLM is pre-trained on the IIT-CDIP Test Collection\n",
      "1.02 [14], which contains more than 6 million scanned documents\n",
      "with 11 million scanned document images. The scanned documents\n",
      "are in a variety of categories, including letter, memo, email, file-\n",
      "folder, form, handwritten, invoice, advertisement, budget, news\n",
      "\n",
      "=>\n",
      "2https://ir.nist.gov/cdip/\n",
      "\n",
      "=>\n",
      "articles, presentation, scientific publication, questionnaire, resume,\n",
      "scientific report, specification, and many others, which is ideal for\n",
      "large-scale self-supervised pre-training. We select three benchmark\n",
      "datasets as the downstream tasks to evaluate the performance of the\n",
      "pre-trained LayoutLM model. The first is the FUNSD dataset3 [10]\n",
      "that is used for spatial layout analysis and form understanding.\n",
      "The second is the SROIE dataset4 for Scanned Receipts Information\n",
      "Extraction. The third is the RVL-CDIP dataset5 [8] for document\n",
      "image classification, which consists of 400,000 grayscale images in\n",
      "16 classes. Experiments illustrate that the pre-trained LayoutLM\n",
      "model significantly outperforms several SOTA pre-trained models\n",
      "on these benchmark datasets, demonstrating the enormous advan-\n",
      "tage for pre-training of text and layout information in document\n",
      "image understanding tasks.\n",
      "The contributions of this paper are summarized as follows:\n",
      "\n",
      "=>\n",
      "• For the first time, textual and layout information from scanned\n",
      "document images is pre-trained in a single framework. Image\n",
      "features are also leveraged to achieve new state-of-the-art\n",
      "results.\n",
      "• LayoutLM uses the masked visual-language model and the\n",
      "multi-label document classification as the training objectives,\n",
      "which significantly outperforms several SOTA pre-trained\n",
      "models in document image understanding tasks.\n",
      "• The code and pre-trained models are publicly available at\n",
      "https://aka.ms/layoutlm for more downstream tasks.\n",
      "\n",
      "=>\n",
      "2\n",
      "LAYOUTLM\n",
      "\n",
      "=>\n",
      "In this section, we briefly review the BERT model, and introduce\n",
      "how we extend to jointly model text and layout information in the\n",
      "LayoutLM framework.\n",
      "\n",
      "=>\n",
      "3https://guillaumejaume.github.io/FUNSD/\n",
      "4https://rrc.cvc.uab.es/?ch=13\n",
      "5https://www.cs.cmu.edu/~aharley/rvl-cdip/\n",
      "\n",
      "=>\n",
      "Text\n",
      "Embeddings\n",
      "\n",
      "=>\n",
      "Position\n",
      "Embeddings (x0)\n",
      "\n",
      "=>\n",
      "Position\n",
      "Embeddings (y0)\n",
      "\n",
      "=>\n",
      "Position\n",
      "Embeddings (x1)\n",
      "\n",
      "=>\n",
      "Position\n",
      "Embeddings (y1)\n",
      "\n",
      "=>\n",
      "E(86)\n",
      "E(117)\n",
      "E(227)\n",
      "E(281)\n",
      "E(303)\n",
      "E(415)\n",
      "E(468)\n",
      "E(556)\n",
      "\n",
      "=>\n",
      "E(138)\n",
      "E(138)\n",
      "E(138)\n",
      "E(138)\n",
      "E(139)\n",
      "E(138)\n",
      "E(139)\n",
      "E(139)\n",
      "\n",
      "=>\n",
      "E(112)\n",
      "E(162)\n",
      "E(277)\n",
      "E(293)\n",
      "E(331)\n",
      "E(464)\n",
      "E(487)\n",
      "E(583)\n",
      "\n",
      "=>\n",
      "E(148)\n",
      "E(148)\n",
      "E(153)\n",
      "E(148)\n",
      "E(149)\n",
      "E(149)\n",
      "E(149)\n",
      "E(150)\n",
      "\n",
      "=>\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "\n",
      "=>\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "\n",
      "=>\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "\n",
      "=>\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "\n",
      "=>\n",
      "E(Date)\n",
      "E(Routed:)\n",
      "E(January)\n",
      "E(11,)\n",
      "E(1994)\n",
      "E(Contract)\n",
      "E(No.)\n",
      "E(4011)\n",
      "\n",
      "=>\n",
      "E(589)\n",
      "\n",
      "=>\n",
      "E(139)\n",
      "\n",
      "=>\n",
      "E(621)\n",
      "\n",
      "=>\n",
      "E(150)\n",
      "\n",
      "=>\n",
      "+\n",
      "\n",
      "=>\n",
      "+\n",
      "\n",
      "=>\n",
      "+\n",
      "\n",
      "=>\n",
      "+\n",
      "\n",
      "=>\n",
      "E(0000)\n",
      "\n",
      "=>\n",
      "E(0)\n",
      "\n",
      "=>\n",
      "E(0)\n",
      "\n",
      "=>\n",
      "E(maxW)\n",
      "\n",
      "=>\n",
      "E(maxH)\n",
      "\n",
      "=>\n",
      "+\n",
      "\n",
      "=>\n",
      "+\n",
      "\n",
      "=>\n",
      "+\n",
      "\n",
      "=>\n",
      "+\n",
      "\n",
      "=>\n",
      "E([CLS])\n",
      "Faster R-CNN\n",
      "\n",
      "=>\n",
      "FC Layers\n",
      "\n",
      "=>\n",
      "Pre-trained LayoutLM\n",
      "\n",
      "=>\n",
      "Pre-built\n",
      "\n",
      "=>\n",
      "OCR/\n",
      "\n",
      "=>\n",
      "PDF \n",
      "Parser\n",
      "\n",
      "=>\n",
      "ROI\n",
      "\n",
      "=>\n",
      "Image\n",
      "Embeddings\n",
      "\n",
      "=>\n",
      "Date\n",
      "Routed:\n",
      "January\n",
      "11,\n",
      "1994\n",
      "Contract\n",
      "No.\n",
      "4011\n",
      "0000\n",
      "[CLS]\n",
      "LayoutLM\n",
      "Embeddings\n",
      "\n",
      "=>\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "\n",
      "=>\n",
      "Downstream Tasks\n",
      "\n",
      "=>\n",
      "Figure 2: An example of LayoutLM, where 2-D layout and image embeddings are integrated into the original BERT architecture.\n",
      "The LayoutLM embeddings and image embeddings from Faster R-CNN work together for downstream tasks.\n",
      "\n",
      "=>\n",
      "2.1\n",
      "The BERT Model\n",
      "\n",
      "=>\n",
      "The BERT model is an attention-based bidirectional language mod-\n",
      "eling approach. It has been verified that the BERT model shows\n",
      "effective knowledge transfer from the self-supervised task with\n",
      "large-scale training data. The architecture of BERT is basically a\n",
      "multi-layer bidirectional Transformer encoder. It accepts a sequence\n",
      "of tokens and stacks multiple layers to produce final representa-\n",
      "tions. In detail, given a set of tokens processed using WordPiece, the\n",
      "input embeddings are computed by summing the corresponding\n",
      "word embeddings, position embeddings, and segment embeddings.\n",
      "Then, these input embeddings are passed through a multi-layer\n",
      "bidirectional Transformer that can generate contextualized repre-\n",
      "sentations with an adaptive attention mechanism.\n",
      "There are two steps in the BERT framework: pre-training and\n",
      "fine-tuning. During the pre-training, the model uses two objectives\n",
      "to learn the language representation: Masked Language Modeling\n",
      "(MLM) and Next Sentence Prediction (NSP), where MLM randomly\n",
      "masks some input tokens and the objective is to recover these\n",
      "masked tokens, and NSP is a binary classification task taking a\n",
      "pair of sentences as inputs and classifying whether they are two\n",
      "consecutive sentences. In the fine-tuning, task-specific datasets are\n",
      "used to update all parameters in an end-to-end way. The BERT\n",
      "model has been successfully applied in a set of NLP tasks.\n",
      "\n",
      "=>\n",
      "2.2\n",
      "The LayoutLM Model\n",
      "\n",
      "=>\n",
      "Although BERT-like models become the state-of-the-art techniques\n",
      "on several challenging NLP tasks, they usually leverage text infor-\n",
      "mation only for any kind of inputs. When it comes to visually rich\n",
      "documents, there is much more information that can be encoded\n",
      "into the pre-trained model. Therefore, we propose to utilize the\n",
      "visually rich information from document layouts and align them\n",
      "with the input texts. Basically, there are two types of features which\n",
      "\n",
      "=>\n",
      "substantially improve the language representation in a visually rich\n",
      "document, which are:\n",
      "\n",
      "=>\n",
      "Document Layout Information. It is evident that the relative po-\n",
      "sitions of words in a document contribute a lot to the semantic\n",
      "representation. Taking form understanding as an example, given a\n",
      "key in a form (e.g., “Passport ID:”), its corresponding value is much\n",
      "more likely on its right or below instead of on the left or above.\n",
      "Therefore, we can embed these relative positions information as\n",
      "2-D position representation. Based on the self-attention mechanism\n",
      "within the Transformer, embedding 2-D position features into the\n",
      "language representation will better align the layout information\n",
      "with the semantic representation.\n",
      "\n",
      "=>\n",
      "Visual Information. Compared with the text information, the\n",
      "visual information is another significantly important feature in doc-\n",
      "ument representations. Typically, documents contain some visual\n",
      "signals to show the importance and priority of document segments.\n",
      "The visual information can be represented by image features and ef-\n",
      "fectively utilized in document representations. For document-level\n",
      "visual features, the whole image can indicate the document layout,\n",
      "which is an essential feature for document image classification. For\n",
      "word-level visual features, styles such as bold, underline, and italic,\n",
      "are also significant hints for the sequence labeling tasks. There-\n",
      "fore, we believe that combining the image features with traditional\n",
      "text representations can bring richer semantic representations to\n",
      "documents.\n",
      "\n",
      "=>\n",
      "2.3\n",
      "Model Architecture\n",
      "\n",
      "=>\n",
      "To take advantage of existing pre-trained models and adapt to\n",
      "document image understanding tasks, we use the BERT architecture\n",
      "as the backbone and add two new input embeddings: a 2-D position\n",
      "embedding and an image embedding.\n",
      "\n",
      "=>\n",
      "2-D Position Embedding. Unlike the position embedding that\n",
      "models the word position in a sequence, 2-D position embedding\n",
      "aims to model the relative spatial position in a document. To repre-\n",
      "sent the spatial position of elements in scanned document images,\n",
      "we consider a document page as a coordinate system with the top-\n",
      "left origin. In this setting, the bounding box can be precisely defined\n",
      "by (x0, y0, x1, y1), where (x0, y0) corresponds to the position of the\n",
      "upper left in the bounding box, and (x1, y1) represents the position\n",
      "of the lower right. We add four position embedding layers with two\n",
      "embedding tables, where the embedding layers representing the\n",
      "same dimension share the same embedding table. This means that\n",
      "we look up the position embedding of x0 and x1 in the embedding\n",
      "table X and lookup y0 and y1 in table Y.\n",
      "\n",
      "=>\n",
      "Image Embedding. To utilize the image feature of a document and\n",
      "align the image feature with the text, we add an image embedding\n",
      "layer to represent image features in language representation. In\n",
      "more detail, with the bounding box of each word from OCR results,\n",
      "we split the image into several pieces, and they have a one-to-one\n",
      "correspondence with the words. We generate the image region\n",
      "features with these pieces of images from the Faster R-CNN [19]\n",
      "model as the token image embeddings. For the [CLS] token, we\n",
      "also use the Faster R-CNN model to produce embeddings using the\n",
      "whole scanned document image as the Region of Interest (ROI) to\n",
      "benefit the downstream tasks which need the representation of the\n",
      "[CLS] token.\n",
      "\n",
      "=>\n",
      "2.4\n",
      "Pre-training LayoutLM\n",
      "\n",
      "=>\n",
      "Task #1: Masked Visual-Language Model. Inspired by the masked\n",
      "language model, we propose the Masked Visual-language Model\n",
      "(MVLM) to learn the language representation with the clues of 2-D\n",
      "position embeddings and text embeddings. During the pre-training,\n",
      "we randomly mask some of the input tokens but keep the corre-\n",
      "sponding 2-D position embeddings, and then the model is trained\n",
      "to predict the masked tokens given the contexts. In this way, the\n",
      "LayoutLM model not only understands the language contexts but\n",
      "also utilizes the corresponding 2-D position information, thereby\n",
      "bridging the gap between the visual and language modalities.\n",
      "\n",
      "=>\n",
      "Task #2: Multi-label Document Classification. For document im-\n",
      "age understanding, many tasks require the model to generate high-\n",
      "quality document-level representations. As the IIT-CDIP Test Col-\n",
      "lection includes multiple tags for each document image, we also\n",
      "use a Multi-label Document Classification (MDC) loss during the\n",
      "pre-training phase. Given a set of scanned documents, we use the\n",
      "document tags to supervise the pre-training process so that the\n",
      "model can cluster the knowledge from different domains and gener-\n",
      "ate better document-level representation. Since the MDC loss needs\n",
      "the label for each document image that may not exist for larger\n",
      "datasets, it is optional during the pre-training and may not be used\n",
      "for pre-training larger models in the future. We will compare the\n",
      "performance of MVLM and MVLM+MDC in Section 3.\n",
      "\n",
      "=>\n",
      "2.5\n",
      "Fine-tuning LayoutLM\n",
      "\n",
      "=>\n",
      "The pre-trained LayoutLM model is fine-tuned on three document\n",
      "image understanding tasks, including a form understanding task, a\n",
      "\n",
      "=>\n",
      "receipt understanding task as well as a document image classifica-\n",
      "tion task. For the form and receipt understanding tasks, LayoutLM\n",
      "predicts {B, I, E, S, O} tags for each token and uses sequential label-\n",
      "ing to detect each type of entity in the dataset. For the document\n",
      "image classification task, LayoutLM predicts the class labels using\n",
      "the representation of the [CLS] token.\n",
      "\n",
      "=>\n",
      "3\n",
      "EXPERIMENTS\n",
      "3.1\n",
      "Pre-training Dataset\n",
      "\n",
      "=>\n",
      "The performance of pre-trained models is largely determined by\n",
      "the scale and quality of datasets. Therefore, we need a large-scale\n",
      "scanned document image dataset to pre-train the LayoutLM model.\n",
      "Our model is pre-trained on the IIT-CDIP Test Collection 1.0, which\n",
      "contains more than 6 million documents, with more than 11 million\n",
      "scanned document images. Moreover, each document has its cor-\n",
      "responding text and metadata stored in XML files. The text is the\n",
      "content produced by applying OCR to document images. The meta-\n",
      "data describes the properties of the document, such as the unique\n",
      "identity and document labels. Although the metadata contains er-\n",
      "roneous and inconsistent tags, the scanned document images in\n",
      "this large-scale dataset are perfectly suitable for pre-training our\n",
      "model.\n",
      "\n",
      "=>\n",
      "3.2\n",
      "Fine-tuning Dataset\n",
      "\n",
      "=>\n",
      "The FUNSD Dataset. We evaluate our approach on the FUNSD\n",
      "dataset for form understanding in noisy scanned documents. This\n",
      "dataset includes 199 real, fully annotated, scanned forms with 9,707\n",
      "semantic entities and 31,485 words. These forms are organized as a\n",
      "list of semantic entities that are interlinked. Each semantic entity\n",
      "comprises a unique identifier, a label (i.e., question, answer, header,\n",
      "or other), a bounding box, a list of links with other entities, and a\n",
      "list of words. The dataset is split into 149 training samples and 50\n",
      "testing samples. We adopt the word-level F1 score as the evaluation\n",
      "metric.\n",
      "\n",
      "=>\n",
      "The SROIE Dataset. We also evaluate our model on the SROIE\n",
      "dataset for receipt information extraction (Task 3). The dataset\n",
      "contains 626 receipts for training and 347 receipts for testing. Each\n",
      "receipt is organized as a list of text lines with bounding boxes. Each\n",
      "receipt is labeled with four types of entities which are {company,\n",
      "date, address, total}. The evaluation metric is the exact match of the\n",
      "entity recognition results in the F1 score.\n",
      "\n",
      "=>\n",
      "The RVL-CDIP Dataset. The RVL-CDIP dataset consists of 400,000\n",
      "grayscale images in 16 classes, with 25,000 images per class. There\n",
      "are 320,000 training images, 40,000 validation images, and 40,000\n",
      "test images. The images are resized, so their largest dimension does\n",
      "not exceed 1,000 pixels. The 16 classes include {letter, form, email,\n",
      "handwritten, advertisement, scientific report, scientific publication,\n",
      "specification, file folder, news article, budget, invoice, presentation,\n",
      "questionnaire, resume, memo}. The evaluation metric is the overall\n",
      "classification accuracy.\n",
      "\n",
      "=>\n",
      "3.3\n",
      "Document Pre-processing\n",
      "\n",
      "=>\n",
      "To utilize the layout information of each document, we need to\n",
      "obtain the location of each token. However, the pre-training dataset\n",
      "(IIT-CDIP Test Collection) only contains pure texts while missing\n",
      "\n",
      "=>\n",
      "their corresponding bounding boxes. In this case, we re-process the\n",
      "scanned document images to obtain the necessary layout informa-\n",
      "tion. Like the original pre-processing in IIT-CDIP Test Collection,\n",
      "we similarly process the dataset by applying OCR to document\n",
      "images. The difference is that we obtain both the recognized words\n",
      "and their corresponding locations in the document image. Thanks\n",
      "to Tesseract6, an open-source OCR engine, we can easily obtain the\n",
      "recognition as well as the 2-D positions. We store the OCR results in\n",
      "hOCR format, a standard specification format which clearly defines\n",
      "the OCR results of one single document image using a hierarchical\n",
      "representation.\n",
      "\n",
      "=>\n",
      "3.4\n",
      "Model Pre-training\n",
      "\n",
      "=>\n",
      "We initialize the weight of LayoutLM model with the pre-trained\n",
      "BERT base model. Specifically, our BASE model has the same ar-\n",
      "chitecture: a 12-layer Transformer with 768 hidden sizes, and 12\n",
      "attention heads, which contains about 113M parameters. Therefore,\n",
      "we use the BERT base model to initialize all modules in our model\n",
      "except the 2-D position embedding layer. For the LARGE setting,\n",
      "our model has a 24-layer Transformer with 1,024 hidden sizes and\n",
      "16 attention heads, which is initialized by the pre-trained BERT\n",
      "LARGE model and contains about 343M parameters. Following [4],\n",
      "we select 15% of the input tokens for prediction. We replace these\n",
      "masked tokens with the [MASK] token 80% of the time, a random to-\n",
      "ken 10% of the time, and an unchanged token 10% of the time. Then,\n",
      "the model predicts the corresponding token with the cross-entropy\n",
      "loss.\n",
      "In addition, we also add the 2-D position embedding layers with\n",
      "four embedding representations (x0, y0, x1, y1), where (x0, y0) cor-\n",
      "responds to the position of the upper left in the bounding box, and\n",
      "(x1, y1) represents the position of the lower right. Considering that\n",
      "the document layout may vary in different page size, we scale the\n",
      "actual coordinate to a “virtual” coordinate: the actual coordinate is\n",
      "scaled to have a value from 0 to 1,000. Furthermore, we also use the\n",
      "ResNet-101 model as the backbone network in the Faster R-CNN\n",
      "model, which is pre-trained on the Visual Genome dataset [12].\n",
      "We train our model on 8 NVIDIA Tesla V100 32GB GPUs with a\n",
      "total batch size of 80. The Adam optimizer is used with an initial\n",
      "learning rate of 5e-5 and a linear decay learning rate schedule. The\n",
      "BASE model takes 80 hours to finish one epoch on 11M documents,\n",
      "while the LARGE model takes nearly 170 hours to finish one epoch.\n",
      "\n",
      "=>\n",
      "3.5\n",
      "Task-specific Fine-tuning\n",
      "\n",
      "=>\n",
      "We evaluate the LayoutLM model on three document image under-\n",
      "standing tasks: Form Understanding, Receipt Understanding,\n",
      "and Document Image Classification. We follow the typical fine-\n",
      "tuning strategy and update all parameters in an end-to-end way on\n",
      "task-specific datasets.\n",
      "\n",
      "=>\n",
      "Form Understanding. This task requires extracting and structur-\n",
      "ing the textual content of forms. It aims to extract key-value pairs\n",
      "from the scanned form images. In more detail, this task includes\n",
      "two sub-tasks: semantic labeling and semantic linking. Semantic\n",
      "labeling is the task of aggregating words as semantic entities and\n",
      "assigning pre-defined labels to them. Semantic linking is the task\n",
      "\n",
      "=>\n",
      "6https://github.com/tesseract-ocr/tesseract\n",
      "\n",
      "=>\n",
      "of predicting the relations between semantic entities. In this work,\n",
      "we focus on the semantic labeling task, while semantic linking\n",
      "is out of the scope. To fine-tune LayoutLM on this task, we treat\n",
      "semantic labeling as a sequence labeling problem. We pass the final\n",
      "representation into a linear layer followed by a softmax layer to\n",
      "predict the label of each token. The model is trained for 100 epochs\n",
      "with a batch size of 16 and a learning rate of 5e-5.\n",
      "\n",
      "=>\n",
      "Receipt Understanding. This task requires filling several pre-\n",
      "defined semantic slots according to the scanned receipt images.\n",
      "For instance, given a set of receipts, we need to fill specific slots (\n",
      "i.g., company, address, date, and total). Different from the form un-\n",
      "derstanding task that requires labeling all matched entities and key-\n",
      "value pairs, the number of semantic slots is fixed with pre-defined\n",
      "keys. Therefore, the model only needs to predict the corresponding\n",
      "values using the sequence labeling method.\n",
      "\n",
      "=>\n",
      "Document Image Classification. Given a visually rich document,\n",
      "this task aims to predict the corresponding category for each doc-\n",
      "ument image. Distinct from the existing image-based approaches,\n",
      "our model includes not only image representations but also text and\n",
      "layout information using the multimodal architecture in LayoutLM.\n",
      "Therefore, our model can combine the text, layout, and image in-\n",
      "formation in a more effective way. To fine-tune our model on this\n",
      "task, we concatenate the output from the LayoutLM model and the\n",
      "whole image embedding, followed by a softmax layer for category\n",
      "prediction. We fine-tune the model for 30 epochs with a batch size\n",
      "of 40 and a learning rate of 2e-5.\n",
      "\n",
      "=>\n",
      "3.6\n",
      "Results\n",
      "\n",
      "=>\n",
      "Form Understanding. We evaluate the form understanding task\n",
      "on the FUNSD dataset. The experiment results are shown in Table 1.\n",
      "We compare the LayoutLM model with two SOTA pre-trained NLP\n",
      "models: BERT and RoBERTa [16]. The BERT BASE model achieves\n",
      "0.603 and while the LARGE model achieves 0.656 in F1. Compared\n",
      "to BERT, the RoBERTa performs much better on this dataset as it is\n",
      "trained using larger data with more epochs. Due to the time limita-\n",
      "tion, we present 4 settings for LayoutLM, which are 500K document\n",
      "pages with 6 epochs, 1M with 6 epochs, 2M with 6 epochs as well\n",
      "as 11M with 2 epochs. It is observed that the LayoutLM model sub-\n",
      "stantially outperforms existing SOTA pre-training baselines. With\n",
      "the BASE architecture, the LayoutLM model with 11M training\n",
      "data achieves 0.7866 in F1, which is much higher than BERT and\n",
      "RoBERTa with the similar size of parameters. In addition, we also\n",
      "add the MDC loss in the pre-training step and it does bring substan-\n",
      "tial improvements on the FUNSD dataset. Finally, the LayoutLM\n",
      "model achieves the best performance of 0.7927 when using the text,\n",
      "layout, and image information at the same time.\n",
      "In addition, we also evaluate the LayoutLM model with different\n",
      "data and epochs on the FUNSD dataset, which is shown in Table 2.\n",
      "For different data settings, we can see that the overall accuracy\n",
      "is monotonically increased as more epochs are trained during the\n",
      "pre-training step. Furthermore, the accuracy is also improved as\n",
      "more data is fed into the LayoutLM model. As the FUNSD dataset\n",
      "contains only 149 images for fine-tuning, the results confirm that\n",
      "the pre-training of text and layout is effective for scanned document\n",
      "understanding especially with low resource settings.\n",
      "\n",
      "=>\n",
      "Modality\n",
      "Model\n",
      "Precision\n",
      "Recall\n",
      "F1\n",
      "#Parameters\n",
      "\n",
      "=>\n",
      "Text only\n",
      "\n",
      "=>\n",
      "BERTBASE\n",
      "0.5469\n",
      "0.671\n",
      "0.6026\n",
      "110M\n",
      "RoBERTaBASE\n",
      "0.6349\n",
      "0.6975\n",
      "0.6648\n",
      "125M\n",
      "BERTLARGE\n",
      "0.6113\n",
      "0.7085\n",
      "0.6563\n",
      "340M\n",
      "RoBERTaLARGE\n",
      "0.678\n",
      "0.7391\n",
      "0.7072\n",
      "355M\n",
      "\n",
      "=>\n",
      "Text + Layout\n",
      "MVLM\n",
      "\n",
      "=>\n",
      "LayoutLMBASE (500K, 6 epochs)\n",
      "0.665\n",
      "0.7355\n",
      "0.6985\n",
      "113M\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.6909\n",
      "0.7735\n",
      "0.7299\n",
      "113M\n",
      "LayoutLMBASE (2M, 6 epochs)\n",
      "0.7377\n",
      "0.782\n",
      "0.7592\n",
      "113M\n",
      "LayoutLMBASE (11M, 2 epochs)\n",
      "0.7597\n",
      "0.8155\n",
      "0.7866\n",
      "113M\n",
      "\n",
      "=>\n",
      "Text + Layout\n",
      "MVLM+MDC\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.7076\n",
      "0.7695\n",
      "0.7372\n",
      "113M\n",
      "LayoutLMBASE (11M, 1 epoch)\n",
      "0.7194\n",
      "0.7780\n",
      "0.7475\n",
      "113M\n",
      "\n",
      "=>\n",
      "Text + Layout\n",
      "MVLM\n",
      "LayoutLMLARGE (1M, 6 epochs)\n",
      "0.7171\n",
      "0.805\n",
      "0.7585\n",
      "343M\n",
      "LayoutLMLARGE (11M, 1 epoch)\n",
      "0.7536\n",
      "0.806\n",
      "0.7789\n",
      "343M\n",
      "\n",
      "=>\n",
      "Text + Layout + Image\n",
      "MVLM\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.7101\n",
      "0.7815\n",
      "0.7441\n",
      "160M\n",
      "LayoutLMBASE (11M, 2 epochs)\n",
      "0.7677\n",
      "0.8195\n",
      "0.7927\n",
      "160M\n",
      "\n",
      "=>\n",
      "Table 1: Model accuracy (Precision, Recall, F1) on the FUNSD dataset\n",
      "\n",
      "=>\n",
      "# Pre-training Data\n",
      "# Pre-training Epochs\n",
      "Precision\n",
      "Recall\n",
      "F1\n",
      "\n",
      "=>\n",
      "500K\n",
      "\n",
      "=>\n",
      "1 epoch\n",
      "0.5779\n",
      "0.6955\n",
      "0.6313\n",
      "2 epochs\n",
      "0.6217\n",
      "0.705\n",
      "0.6607\n",
      "3 epochs\n",
      "0.6304\n",
      "0.718\n",
      "0.6713\n",
      "4 epochs\n",
      "0.6383\n",
      "0.7175\n",
      "0.6756\n",
      "5 epochs\n",
      "0.6568\n",
      "0.734\n",
      "0.6933\n",
      "6 epochs\n",
      "0.665\n",
      "0.7355\n",
      "0.6985\n",
      "\n",
      "=>\n",
      "1M\n",
      "\n",
      "=>\n",
      "1 epoch\n",
      "0.6156\n",
      "0.7005\n",
      "0.6552\n",
      "2 epochs\n",
      "0.6545\n",
      "0.737\n",
      "0.6933\n",
      "3 epochs\n",
      "0.6794\n",
      "0.762\n",
      "0.7184\n",
      "4 epochs\n",
      "0.6812\n",
      "0.766\n",
      "0.7211\n",
      "5 epochs\n",
      "0.6863\n",
      "0.7625\n",
      "0.7224\n",
      "6 epochs\n",
      "0.6909\n",
      "0.7735\n",
      "0.7299\n",
      "\n",
      "=>\n",
      "2M\n",
      "\n",
      "=>\n",
      "1 epoch\n",
      "0.6599\n",
      "0.7355\n",
      "0.6957\n",
      "2 epochs\n",
      "0.6938\n",
      "0.759\n",
      "0.7249\n",
      "3 epochs\n",
      "0.6915\n",
      "0.7655\n",
      "0.7266\n",
      "4 epochs\n",
      "0.7081\n",
      "0.781\n",
      "0.7427\n",
      "5 epochs\n",
      "0.7228\n",
      "0.7875\n",
      "0.7538\n",
      "6 epochs\n",
      "0.7377\n",
      "0.782\n",
      "0.7592\n",
      "\n",
      "=>\n",
      "11M\n",
      "1 epoch\n",
      "0.7464\n",
      "0.7815\n",
      "0.7636\n",
      "2 epochs\n",
      "0.7597\n",
      "0.8155\n",
      "0.7866\n",
      "\n",
      "=>\n",
      "Table 2: LayoutLMBASE (Text + Layout, MVLM) accuracy with different data and epochs on the FUNSD dataset\n",
      "\n",
      "=>\n",
      "Furthermore, we compare different initialization methods for\n",
      "the LayoutLM model including from scratch, BERT and RoBERTa.\n",
      "The results in Table 3 show that the LayoutLMBASE model initial-\n",
      "ized with RoBERTaBASE outperforms BERTBASE by 2.1 points in F1.\n",
      "For the LARGE setting, the LayoutLMLARGE model initialized with\n",
      "RoBERTaLARGE further improve 1.3 points over the BERTLARGE\n",
      "model. We will pre-train more models with RoBERTa as the initial-\n",
      "ization in the future, especially for the LARGE settings.\n",
      "\n",
      "=>\n",
      "Receipt Understanding. We evaluate the receipt understanding\n",
      "task using the SROIE dataset. The results are shown in Table 4. As\n",
      "we only test the performance of the Key Information Extraction\n",
      "task in SROIE, we would like to eliminate the effect of incorrect\n",
      "OCR results. Therefore, we pre-process the training data by using\n",
      "the ground truth OCR and run a set of experiments using the base-\n",
      "line models (BERT & RoBERTa) as well as the LayoutLM model.\n",
      "The results show that the LayoutLMLARGE model trained with 11M\n",
      "\n",
      "=>\n",
      "Initialization\n",
      "Model\n",
      "Precision\n",
      "Recall\n",
      "F1\n",
      "\n",
      "=>\n",
      "SCRATCH\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.5630\n",
      "0.6728\n",
      "0.6130\n",
      "BERTBASE\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.6909\n",
      "0.7735\n",
      "0.7299\n",
      "RoBERTaBASE\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.7173\n",
      "0.7888\n",
      "0.7514\n",
      "SCRATCH\n",
      "LayoutLMLARGE (11M, 1 epoch)\n",
      "0.6845\n",
      "0.7804\n",
      "0.7293\n",
      "BERTLARGE\n",
      "LayoutLMLARGE (11M, 1 epoch)\n",
      "0.7536\n",
      "0.8060\n",
      "0.7789\n",
      "RoBERTaLARGE\n",
      "LayoutLMLARGE (11M, 1 epoch)\n",
      "0.7681\n",
      "0.8188\n",
      "0.7926\n",
      "\n",
      "=>\n",
      "Table 3: Different initialization methods for BASE and LARGE (Text + Layout, MVLM)\n",
      "\n",
      "=>\n",
      "document images achieve an F1 score of 0.9524, which is signifi-\n",
      "cantly better than the first place in the competition leaderboard.\n",
      "This result also verifies that the pre-trained LayoutLM not only per-\n",
      "forms well on the in-domain dataset (FUNSD) but also outperforms\n",
      "several strong baselines on the out-of-domain dataset like SROIE.\n",
      "\n",
      "=>\n",
      "Document Image Classification. Finally, we evaluate the docu-\n",
      "ment image classification task using the RVL-CDIP dataset. Doc-\n",
      "ument images are different from other natural images as most of\n",
      "the content in document images are texts in a variety of styles and\n",
      "layouts. Traditionally, image-based classification models with pre-\n",
      "training perform much better than the text-based models, which\n",
      "is shown in Table 5. We can see that either BERT or RoBERTa\n",
      "underperforms the image-based approaches, illustrating that text\n",
      "information is not sufficient for this task, and it still needs layout\n",
      "and image features. We address this issue by using the LayoutLM\n",
      "model for this task. Results show that, even without the image\n",
      "features, LayoutLM still outperforms the single model of the image-\n",
      "based approaches. After integrating the image embeddings, the\n",
      "LayoutLM achieves the accuracy of 94.42%, which is significantly\n",
      "better than several SOTA baselines for document image classifi-\n",
      "cation. It is observed that our model performs best in the \"email\"\n",
      "category while performs worst in the \"form\" category. We will\n",
      "further investigate how to take advantage of both pre-trained Lay-\n",
      "outLM and image models, as well as involve image information in\n",
      "the pre-training step for the LayoutLM model.\n",
      "\n",
      "=>\n",
      "4\n",
      "RELATED WORK\n",
      "\n",
      "=>\n",
      "The research of Document Analysis and Recognition (DAR) dates\n",
      "to the early 1990s. The mainstream approaches can be divided\n",
      "into three categories: rule-based approaches, conventional machine\n",
      "learning approaches, and deep learning approaches.\n",
      "\n",
      "=>\n",
      "4.1\n",
      "Rule-based Approaches\n",
      "\n",
      "=>\n",
      "The rule-based approaches [6, 13, 18, 23] contain two types of anal-\n",
      "ysis methods: bottom-up and top-down. Bottom-up methods [5,\n",
      "13, 23] usually detect the connected components of black pixels as\n",
      "the basic computational units in document images, and the docu-\n",
      "ment segmentation process is to combine them into higher-level\n",
      "structures through different heuristics and label them according\n",
      "to different structural features. Docstrum algorithm [18] is among\n",
      "the earliest successful bottom-up algorithms that are based on the\n",
      "connected component analysis. It groups connected components\n",
      "on a polar structure to derive the final segmentation. [23] use a\n",
      "special distance-metric between different components to construct\n",
      "\n",
      "=>\n",
      "a physical page structure. They further reduced the time complexity\n",
      "by using heuristics and path compression algorithms.\n",
      "The top-down methods often recursively split a page into columns,\n",
      "blocks, text lines, and tokens. [6] propose replacing the basic unit\n",
      "with the black pixels from all the pixels, and the method decom-\n",
      "posed the document using the recursive the X-Y cut algorithm to\n",
      "establish an X-Y tree, which makes complex documents decompose\n",
      "more easily. Although these methods perform well on some doc-\n",
      "uments, they require extensive human efforts to figure out better\n",
      "rules, while sometimes failing to generalize to documents from\n",
      "other sources. Therefore, it is inevitable to leverage machine learn-\n",
      "ing approaches in the DAR research.\n",
      "\n",
      "=>\n",
      "4.2\n",
      "Machine Learning Approaches\n",
      "\n",
      "=>\n",
      "With the development of conventional machine learning, statistical\n",
      "machine learning approaches [17, 22] have become the mainstream\n",
      "for document segmentation tasks during the past decade. [22] con-\n",
      "sider the layout information of a document as a parsing problem,\n",
      "and globally search the optimal parsing tree based on a grammar-\n",
      "based loss function. They utilize a machine learning approach to\n",
      "select features and train all parameters during the parsing process.\n",
      "Meanwhile, artificial neural networks [17] have been extensively\n",
      "applied to document analysis and recognition. Most efforts have\n",
      "been devoted to the recognition of isolated handwritten and printed\n",
      "characters with widely recognized successful results. In addition to\n",
      "the ANN model, SVM and GMM [27] have been used in document\n",
      "layout analysis tasks. For machine learning approaches, they are\n",
      "usually time-consuming to design manually crafted features and\n",
      "difficult to obtain a highly abstract semantic context. In addition,\n",
      "these methods usually relied on visual cues but ignored textual\n",
      "information.\n",
      "\n",
      "=>\n",
      "4.3\n",
      "Deep Learning Approaches\n",
      "\n",
      "=>\n",
      "Recently, deep learning methods have become the mainstream and\n",
      "de facto standard for many machine learning problems. Theoreti-\n",
      "cally, they can fit any arbitrary functions through the stacking of\n",
      "multi-layer neural networks and have been verified to be effective\n",
      "in many research areas. [28] treat the document semantic structure\n",
      "extraction task as a pixel-by-pixel classification problem. They pro-\n",
      "pose a multimodal neural network that considers visual and textual\n",
      "information, while the limitation of this work is that they only\n",
      "used the network to assist heuristic algorithms to classify candidate\n",
      "bounding boxes rather than an end-to-end approach. [26] propose a\n",
      "lightweight model of document layout analysis for mobile and cloud\n",
      "services. The model uses one-dimensional information of images for\n",
      "\n",
      "=>\n",
      "Modality\n",
      "Model\n",
      "Precision\n",
      "Recall\n",
      "F1\n",
      "#Parameters\n",
      "\n",
      "=>\n",
      "Text only\n",
      "\n",
      "=>\n",
      "BERTBASE\n",
      "0.9099\n",
      "0.9099\n",
      "0.9099\n",
      "110M\n",
      "RoBERTaBASE\n",
      "0.9107\n",
      "0.9107\n",
      "0.9107\n",
      "125M\n",
      "BERTLARGE\n",
      "0.9200\n",
      "0.9200\n",
      "0.9200\n",
      "340M\n",
      "RoBERTaLARGE\n",
      "0.9280\n",
      "0.9280\n",
      "0.9280\n",
      "355M\n",
      "\n",
      "=>\n",
      "Text + Layout\n",
      "MVLM\n",
      "\n",
      "=>\n",
      "LayoutLMBASE (500K, 6 epochs)\n",
      "0.9388\n",
      "0.9388\n",
      "0.9388\n",
      "113M\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.9380\n",
      "0.9380\n",
      "0.9380\n",
      "113M\n",
      "LayoutLMBASE (2M, 6 epochs)\n",
      "0.9431\n",
      "0.9431\n",
      "0.9431\n",
      "113M\n",
      "LayoutLMBASE (11M, 2 epochs)\n",
      "0.9438\n",
      "0.9438\n",
      "0.9438\n",
      "113M\n",
      "\n",
      "=>\n",
      "Text + Layout\n",
      "MVLM+MDC\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.9402\n",
      "0.9402\n",
      "0.9402\n",
      "113M\n",
      "LayoutLMBASE (11M, 1 epoch)\n",
      "0.9460\n",
      "0.9460\n",
      "0.9460\n",
      "113M\n",
      "\n",
      "=>\n",
      "Text + Layout\n",
      "MVLM\n",
      "LayoutLMLARGE (1M, 6 epochs)\n",
      "0.9416\n",
      "0.9416\n",
      "0.9416\n",
      "343M\n",
      "LayoutLMLARGE (11M, 1 epoch)\n",
      "0.9524\n",
      "0.9524\n",
      "0.9524\n",
      "343M\n",
      "\n",
      "=>\n",
      "Text + Layout + Image\n",
      "MVLM\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.9416\n",
      "0.9416\n",
      "0.9416\n",
      "160M\n",
      "LayoutLMBASE (11M, 2 epochs)\n",
      "0.9467\n",
      "0.9467\n",
      "0.9467\n",
      "160M\n",
      "\n",
      "=>\n",
      "Baseline\n",
      "Ranking 1st in SROIE\n",
      "0.9402\n",
      "0.9402\n",
      "0.9402\n",
      "-\n",
      "\n",
      "=>\n",
      "Table 4: Model accuracy (Precision, Recall, F1) on the SROIE dataset\n",
      "\n",
      "=>\n",
      "Modality\n",
      "Model\n",
      "Accuracy\n",
      "#Parameters\n",
      "\n",
      "=>\n",
      "Text only\n",
      "\n",
      "=>\n",
      "BERTBASE\n",
      "89.81%\n",
      "110M\n",
      "RoBERTaBASE\n",
      "90.06%\n",
      "125M\n",
      "BERTLARGE\n",
      "89.92%\n",
      "340M\n",
      "RoBERTaLARGE\n",
      "90.11%\n",
      "355M\n",
      "\n",
      "=>\n",
      "Text + Layout\n",
      "MVLM\n",
      "\n",
      "=>\n",
      "LayoutLMBASE (500K, 6 epochs)\n",
      "91.25%\n",
      "113M\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "91.48%\n",
      "113M\n",
      "LayoutLMBASE (2M, 6 epochs)\n",
      "91.65%\n",
      "113M\n",
      "LayoutLMBASE (11M, 2 epochs)\n",
      "91.78%\n",
      "113M\n",
      "\n",
      "=>\n",
      "Text + Layout\n",
      "MVLM+MDC\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "91.74%\n",
      "113M\n",
      "LayoutLMBASE (11M, 1 epoch)\n",
      "91.78%\n",
      "113M\n",
      "\n",
      "=>\n",
      "Text + Layout\n",
      "MVLM\n",
      "LayoutLMLARGE (1M, 6 epochs)\n",
      "91.88%\n",
      "343M\n",
      "LayoutLMLARGE (11M, 1 epoch)\n",
      "91.90%\n",
      "343M\n",
      "\n",
      "=>\n",
      "Text + Layout + Image\n",
      "MVLM\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "94.31%\n",
      "160M\n",
      "LayoutLMBASE (11M, 2 epochs)\n",
      "94.42%\n",
      "160M\n",
      "\n",
      "=>\n",
      "Baselines\n",
      "\n",
      "=>\n",
      "VGG-16 [1]\n",
      "90.97%\n",
      "-\n",
      "Stacked CNN Single [2]\n",
      "91.11%\n",
      "-\n",
      "Stacked CNN Ensemble [2]\n",
      "92.21%\n",
      "-\n",
      "InceptionResNetV2 [25]\n",
      "92.63%\n",
      "-\n",
      "LadderNet [20]\n",
      "92.77%\n",
      "-\n",
      "Multimodal Single [3]\n",
      "93.03%\n",
      "-\n",
      "Multimodal Ensemble [3]\n",
      "93.07%\n",
      "-\n",
      "\n",
      "=>\n",
      "Table 5: Classification accuracy on the RVL-CDIP dataset\n",
      "\n",
      "=>\n",
      "inference and compares it with the model using two-dimensional in-\n",
      "formation, achieving comparable accuracy in the experiments. [11]\n",
      "make use of a fully convolutional encoder-decoder network that\n",
      "predicts a segmentation mask and bounding boxes, and the model\n",
      "significantly outperforms approaches based on sequential text or\n",
      "document images. [24] incorporate contextual information into the\n",
      "\n",
      "=>\n",
      "Faster R-CNN model that involves the inherently localized nature\n",
      "of article contents to improve region detection performance.\n",
      "Existing deep learning approaches for DAR usually confront\n",
      "two limitations: (1) The models often rely on limited labeled data\n",
      "while leaving a large amount of unlabeled data unused. (2) Current\n",
      "deep learning models usually leverage pre-trained CV models or\n",
      "\n",
      "=>\n",
      "NLP models, but do not consider the joint pre-training of text and\n",
      "layout. LayoutLM addresses these two limitations and achieves\n",
      "much better performance compared with the previous baselines.\n",
      "\n",
      "=>\n",
      "5\n",
      "CONCLUSION AND FUTURE WORK\n",
      "\n",
      "=>\n",
      "We present LayoutLM, a simple yet effective pre-training technique\n",
      "with text and layout information in a single framework. Based on\n",
      "the Transformer architecture as the backbone, LayoutLM takes\n",
      "advantage of multimodal inputs, including token embeddings, lay-\n",
      "out embeddings, and image embeddings. Meanwhile, the model\n",
      "can be easily trained in a self-supervised way based on large scale\n",
      "unlabeled scanned document images. We evaluate the LayoutLM\n",
      "model on three tasks: form understanding, receipt understanding,\n",
      "and scanned document image classification. Experiments show\n",
      "that LayoutLM substantially outperforms several SOTA pre-trained\n",
      "models in these tasks.\n",
      "For future research, we will investigate pre-training models with\n",
      "more data and more computation resources. In addition, we will\n",
      "also train LayoutLM using the LARGE architecture with text and\n",
      "layout, as well as involving image embeddings in the pre-training\n",
      "step. Furthermore, we will explore new network architectures and\n",
      "other self-supervised training objectives that may further unlock\n",
      "the power of LayoutLM.\n",
      "\n",
      "=>\n",
      "REFERENCES\n",
      "\n",
      "=>\n",
      "[1] Muhammad Zeshan Afzal, Andreas Kölsch, Sheraz Ahmed, and Marcus Liwicki.\n",
      "2017. Cutting the Error by Half: Investigation of Very Deep CNN and Advanced\n",
      "Training Strategies for Document Image Classification. 2017 14th IAPR Inter-\n",
      "national Conference on Document Analysis and Recognition (ICDAR) 01 (2017),\n",
      "883–888.\n",
      "[2] Arindam Das, Saikat Roy, and Ujjwal Bhattacharya. 2018. Document Image\n",
      "Classification with Intra-Domain Transfer Learning and Stacked Generalization\n",
      "of Deep Convolutional Neural Networks. 2018 24th International Conference on\n",
      "Pattern Recognition (ICPR) (2018), 3180–3185.\n",
      "[3] Tyler Dauphinee, Nikunj Patel, and Mohammad Mehdi Rashidi. 2019. Modular\n",
      "Multimodal Architecture for Document Classification. ArXiv abs/1912.04376\n",
      "(2019).\n",
      "[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\n",
      "Pre-training of Deep Bidirectional Transformers for Language Understanding. In\n",
      "Proceedings of the 2019 Conference of the North American Chapter of the Association\n",
      "for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\n",
      "Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota,\n",
      "4171–4186. https://doi.org/10.18653/v1/N19-1423\n",
      "[5] Jaekyu Ha, Robert M Haralick, and Ihsin T Phillips. 1995. Document page\n",
      "decomposition by the bounding-box project. In Proceedings of 3rd International\n",
      "Conference on Document Analysis and Recognition, Vol. 2. IEEE, 1119–1122.\n",
      "[6] Jaekyu Ha, Robert M Haralick, and Ihsin T Phillips. 1995. Recursive XY cut using\n",
      "bounding boxes of connected components. In Proceedings of 3rd International\n",
      "Conference on Document Analysis and Recognition, Vol. 2. IEEE, 952–955.\n",
      "[7] Leipeng Hao, Liangcai Gao, Xiaohan Yi, and Zhi Tang. 2016. A Table Detection\n",
      "Method for PDF Documents Based on Convolutional Neural Networks. 2016 12th\n",
      "IAPR Workshop on Document Analysis Systems (DAS) (2016), 287–292.\n",
      "[8] Adam W. Harley, Alex Ufkes, and Konstantinos G. Derpanis. 2015. Evaluation of\n",
      "deep convolutional nets for document image classification and retrieval. 2015\n",
      "13th International Conference on Document Analysis and Recognition (ICDAR)\n",
      "(2015), 991–995.\n",
      "[9] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. 2017. Mask\n",
      "R-CNN. CoRR abs/1703.06870 (2017). arXiv:1703.06870 http://arxiv.org/abs/1703.\n",
      "06870\n",
      "[10] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. 2019. FUNSD:\n",
      "A Dataset for Form Understanding in Noisy Scanned Documents. 2019 Interna-\n",
      "tional Conference on Document Analysis and Recognition Workshops (ICDARW) 2\n",
      "(2019), 1–6.\n",
      "[11] Anoop R Katti, Christian Reisswig, Cordula Guder, Sebastian Brarda, Steffen\n",
      "Bickel, Johannes Höhne, and Jean Baptiste Faddoul. 2018. Chargrid: Towards\n",
      "Understanding 2D Documents. In Proceedings of the 2018 Conference on Em-\n",
      "pirical Methods in Natural Language Processing. Association for Computational\n",
      "Linguistics, Brussels, Belgium, 4459–4469. https://doi.org/10.18653/v1/D18-1476\n",
      "\n",
      "=>\n",
      "[12] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua\n",
      "Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael\n",
      "Bernstein, and Li Fei-Fei. 2016. Visual Genome: Connecting Language and Vision\n",
      "Using Crowdsourced Dense Image Annotations. https://arxiv.org/abs/1602.07332\n",
      "[13] Frank Lebourgeois, Z Bublinski, and H Emptoz. 1992. A fast and efficient method\n",
      "for extracting text paragraphs and graphics from unconstrained documents. In\n",
      "Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol. II.\n",
      "Conference B: Pattern Recognition Methodology and Systems. IEEE, 272–276.\n",
      "[14] D. Lewis, G. Agam, S. Argamon, O. Frieder, D. Grossman, and J. Heard. 2006.\n",
      "Building a Test Collection for Complex Document Information Processing. In\n",
      "Proceedings of the 29th Annual International ACM SIGIR Conference on Research\n",
      "and Development in Information Retrieval (Seattle, Washington, USA) (SIGIR ’06).\n",
      "ACM, New York, NY, USA, 665–666. https://doi.org/10.1145/1148170.1148307\n",
      "[15] Xiaojing Liu, Feiyu Gao, Qiong Zhang, and Huasha Zhao. 2019. Graph Convolu-\n",
      "tion for Multimodal Information Extraction from Visually Rich Documents. In\n",
      "Proceedings of the 2019 Conference of the North American Chapter of the Association\n",
      "for Computational Linguistics: Human Language Technologies, Volume 2 (Indus-\n",
      "try Papers). Association for Computational Linguistics, Minneapolis, Minnesota,\n",
      "32–39. https://doi.org/10.18653/v1/N19-2005\n",
      "[16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\n",
      "Levy, Mike Lewis, Luke S. Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\n",
      "Robustly Optimized BERT Pretraining Approach. ArXiv abs/1907.11692 (2019).\n",
      "[17] S. Marinai, M. Gori, and G. Soda. 2005. Artificial neural networks for document\n",
      "analysis and recognition. IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence 27, 1 (Jan 2005), 23–35. https://doi.org/10.1109/TPAMI.2005.4\n",
      "[18] L. O’Gorman. 1993. The document spectrum for page layout analysis. IEEE\n",
      "Transactions on Pattern Analysis and Machine Intelligence 15, 11 (Nov 1993), 1162–\n",
      "1173. https://doi.org/10.1109/34.244677\n",
      "[19] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN:\n",
      "Towards Real-Time Object Detection with Region Proposal Networks. IEEE\n",
      "Transactions on Pattern Analysis and Machine Intelligence 39 (2015), 1137–1149.\n",
      "[20] Ritesh Sarkhel and Arnab Nandi. 2019. Deterministic Routing between Lay-\n",
      "out Abstractions for Multi-Scale Classification of Visually Rich Documents. In\n",
      "Proceedings of the Twenty-Eighth International Joint Conference on Artificial In-\n",
      "telligence, IJCAI-19. International Joint Conferences on Artificial Intelligence\n",
      "Organization, 3360–3366. https://doi.org/10.24963/ijcai.2019/466\n",
      "[21] Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed.\n",
      "2017. DeepDeSRT: Deep Learning for Detection and Structure Recognition of\n",
      "Tables in Document Images. 2017 14th IAPR International Conference on Document\n",
      "Analysis and Recognition (ICDAR) 01 (2017), 1162–1167.\n",
      "[22] Michael Shilman, Percy Liang, and Paul Viola. 2005. Learning nongenerative\n",
      "grammatical models for document analysis. In Tenth IEEE International Conference\n",
      "on Computer Vision (ICCV’05) Volume 1, Vol. 2. IEEE, 962–969.\n",
      "[23] Anikó Simon, J-C Pret, and A Peter Johnson. 1997. A fast algorithm for bottom-up\n",
      "document layout analysis. IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence 19, 3 (1997), 273–277.\n",
      "[24] Carlos Soto and Shinjae Yoo. 2019. Visual Detection with Context for Document\n",
      "Layout Analysis. In Proceedings of the 2019 Conference on Empirical Methods in\n",
      "Natural Language Processing and the 9th International Joint Conference on Natural\n",
      "Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics,\n",
      "Hong Kong, China, 3462–3468. https://doi.org/10.18653/v1/D19-1348\n",
      "[25] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alex Alemi. 2016.\n",
      "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learn-\n",
      "ing. In AAAI.\n",
      "[26] Matheus Palhares Viana and Dário Augusto Borges Oliveira. 2017. Fast CNN-\n",
      "Based Document Layout Analysis. 2017 IEEE International Conference on Computer\n",
      "Vision Workshops (ICCVW) (2017), 1173–1180.\n",
      "[27] H. Wei, M. Baechler, F. Slimane, and R. Ingold. 2013. Evaluation of SVM, MLP\n",
      "and GMM Classifiers for Layout Analysis of Historical Documents. In 2013 12th\n",
      "International Conference on Document Analysis and Recognition. 1220–1224. https:\n",
      "//doi.org/10.1109/ICDAR.2013.247\n",
      "[28] Xiaowei Yang, Ersin Yumer, Paul Asente, Mike Kraley, Daniel Kifer, and C. Lee\n",
      "Giles. 2017. Learning to Extract Semantic Structure from Documents Using\n",
      "Multimodal Fully Convolutional Neural Networks. 2017 IEEE Conference on\n",
      "Computer Vision and Pattern Recognition (CVPR) (2017), 4342–4351.\n",
      "[29] Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. 2019. PubLayNet: largest\n",
      "dataset ever for document layout analysis. ArXiv abs/1908.07836 (2019).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def extract_text_from_pdf(url):\n",
    "    response = requests.get(url)\n",
    "    pdf_content = BytesIO(response.content)\n",
    "    document = fitz.Document(stream=pdf_content, filetype=\"pdf\")\n",
    "\n",
    "    # Extract text from each page\n",
    "    doc_blocks = []\n",
    "    for page_num in range(document.page_count):\n",
    "        page = document.load_page(page_num)\n",
    "        page_blocks = page.get_text(\"blocks\")\n",
    "        for block in page_blocks:\n",
    "            doc_blocks.append(block[4])\n",
    "\n",
    "    return doc_blocks\n",
    "\n",
    "# Example usage\n",
    "# pdf_path = \"../data/RAG.pdf\"\n",
    "# pdf_path = \"../data/LayoutLM.pdf\"\n",
    "url = \"https://arxiv.org/pdf/1912.13318\"\n",
    "doc_blocks = extract_text_from_pdf(url)\n",
    "\n",
    "for block in doc_blocks:\n",
    "    print(\"=>\")\n",
    "    print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 171\n",
      "=>\n",
      " Pre-training techniques have been verified successfully in a vari-\n",
      "ety of NLP tasks in recent years. Despite the widespread use of\n",
      "pre-training models for NLP applications, they almost exclusively\n",
      "focus on text-level manipulation, while neglecting layout and style\n",
      "information that is vital for document image understanding. In\n",
      "this paper, we propose the LayoutLM to jointly model interactions\n",
      "between text and layout information across scanned document\n",
      "images, which is beneficial for a great number of real-world doc-\n",
      "ument image understanding tasks such as information extraction\n",
      "from scanned documents. Furthermore, we also leverage image\n",
      "features to incorporate words’ visual information into LayoutLM.\n",
      "To the best of our knowledge, this is the first time that text and\n",
      "layout are jointly learned in a single framework for document-\n",
      "level pre-training. It achieves new state-of-the-art results in several\n",
      "downstream tasks, including form understanding (from 70.72 to\n",
      "79.27), receipt understanding (from 94.02 to 95.24) and document\n",
      "image classification (from 93.07 to 94.42). The code and pre-trained\n",
      "LayoutLM models are publicly available at https://aka.ms/layoutlm.\n",
      "\n",
      "=>\n",
      " CCS CONCEPTS\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " • Information systems →Business intelligence; • Computing\n",
      "methodologies →Information extraction; Transfer learning;\n",
      "• Applied computing →Document analysis.\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " KEYWORDS\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " LayoutLM; pre-trained models; document image understanding\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " ACM Reference Format:\n",
      "Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming\n",
      "Zhou. 2020. LayoutLM: Pre-training of Text and Layout for Document\n",
      "Image Understanding. In Proceedings of the 26th ACM SIGKDD Conference\n",
      "on Knowledge Discovery and Data Mining (KDD ’20), August 23–27, 2020,\n",
      "Virtual Event, CA, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/\n",
      "10.1145/3394486.3403172\n",
      "\n",
      "=>\n",
      " ∗Equal contributions during internship at Microsoft Research Asia.\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior specific permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "KDD ’20, August 23–27, 2020, Virtual Event, CA, USA\n",
      "© 2020 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-7998-4/20/08...$15.00\n",
      "https://doi.org/10.1145/3394486.3403172\n",
      "\n",
      "=>\n",
      " 1\n",
      "INTRODUCTION\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Document AI, or Document Intelligence1, is a relatively new re-\n",
      "search topic that refers techniques for automatically reading, under-\n",
      "standing, and analyzing business documents. Business documents\n",
      "are files that provide details related to a company’s internal and\n",
      "external transactions, which are shown in Figure 1. They may be\n",
      "digital-born, occurring as electronic files, or they may be in scanned\n",
      "form that comes from written or printed on paper. Some common\n",
      "examples of business documents include purchase orders, financial\n",
      "reports, business emails, sales agreements, vendor contracts, letters,\n",
      "invoices, receipts, resumes, and many others. Business documents\n",
      "are critical to a company’s efficiency and productivity. The exact\n",
      "format of a business document may vary, but the information is\n",
      "usually presented in natural language and can be organized in a\n",
      "variety of ways from plain text, multi-column layouts, and a wide\n",
      "variety of tables/forms/figures. Understanding business documents\n",
      "is a very challenging task due to the diversity of layouts and formats,\n",
      "poor quality of scanned document images as well as the complexity\n",
      "of template structures.\n",
      "Nowadays, many companies extract data from business docu-\n",
      "ments through manual efforts that are time-consuming and expen-\n",
      "sive, meanwhile requiring manual customization or configuration.\n",
      "Rules and workflows for each type of document often need to be\n",
      "hard-coded and updated with changes to the specific format or\n",
      "when dealing with multiple formats. To address these problems,\n",
      "document AI models and algorithms are designed to automatically\n",
      "classify, extract, and structuralize information from business doc-\n",
      "uments, accelerating automated document processing workflows.\n",
      "Contemporary approaches for document AI are usually built upon\n",
      "deep neural networks from a computer vision perspective or a natu-\n",
      "ral language processing perspective, or a combination of them. Early\n",
      "attempts usually focused on detecting and analyzing certain parts\n",
      "of a document, such as tabular areas. [7] were the first to propose a\n",
      "table detection method for PDF documents based on Convolutional\n",
      "Neural Networks (CNN). After that, [21, 24, 29] also leveraged more\n",
      "advanced Faster R-CNN model [19] or Mask R-CNN model [9] to\n",
      "further improve the accuracy of document layout analysis. In addi-\n",
      "tion, [28] presented an end-to-end, multimodal, fully convolutional\n",
      "network for extracting semantic structures from document images,\n",
      "taking advantage of text embeddings from pre-trained NLP models.\n",
      "More recently, [15] introduced a Graph Convolutional Networks\n",
      "(GCN) based model to combine textual and visual information for\n",
      "\n",
      "=>\n",
      " 1https://sites.google.com/view/di2019\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " arXiv:1912.13318v5  [cs.CL]  16 Jun 2020\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " (a)\n",
      "(b)\n",
      "(c)\n",
      "(d)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Figure 1: Scanned images of business documents with different layouts and formats\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " information extraction from business documents. Although these\n",
      "models have made significant progress in the document AI area\n",
      "with deep neural networks, most of these methods confront two\n",
      "limitations: (1) They rely on a few human-labeled training samples\n",
      "without fully exploring the possibility of using large-scale unla-\n",
      "beled training samples. (2) They usually leverage either pre-trained\n",
      "CV models or NLP models, but do not consider a joint training of\n",
      "textual and layout information. Therefore, it is important to inves-\n",
      "tigate how self-supervised pre-training of text and layout may help\n",
      "in the document AI area.\n",
      "To this end, we propose LayoutLM, a simple yet effective pre-\n",
      "training method of text and layout for document image understand-\n",
      "ing tasks. Inspired by the BERT model [4], where input textual\n",
      "information is mainly represented by text embeddings and position\n",
      "embeddings, LayoutLM further adds two types of input embeddings:\n",
      "(1) a 2-D position embedding that denotes the relative position of\n",
      "a token within a document; (2) an image embedding for scanned\n",
      "token images within a document. The architecture of LayoutLM is\n",
      "shown in Figure 2. We add these two input embeddings because\n",
      "the 2-D position embedding can capture the relationship among\n",
      "tokens within a document, meanwhile the image embedding can\n",
      "capture some appearance features such as font directions, types,\n",
      "and colors. In addition, we adopt a multi-task learning objective for\n",
      "LayoutLM, including a Masked Visual-Language Model (MVLM)\n",
      "loss and a Multi-label Document Classification (MDC) loss, which\n",
      "further enforces joint pre-training for text and layout. In this work,\n",
      "our focus is the document pre-training based on scanned docu-\n",
      "ment images, while digital-born documents are less challenging\n",
      "because they can be considered as a special case where OCR is\n",
      "not required, thus they are out of the scope of this paper. Specifi-\n",
      "cally, the LayoutLM is pre-trained on the IIT-CDIP Test Collection\n",
      "1.02 [14], which contains more than 6 million scanned documents\n",
      "with 11 million scanned document images. The scanned documents\n",
      "are in a variety of categories, including letter, memo, email, file-\n",
      "folder, form, handwritten, invoice, advertisement, budget, news\n",
      "\n",
      "=>\n",
      " 2https://ir.nist.gov/cdip/\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " articles, presentation, scientific publication, questionnaire, resume,\n",
      "scientific report, specification, and many others, which is ideal for\n",
      "large-scale self-supervised pre-training. We select three benchmark\n",
      "datasets as the downstream tasks to evaluate the performance of the\n",
      "pre-trained LayoutLM model. The first is the FUNSD dataset3 [10]\n",
      "that is used for spatial layout analysis and form understanding.\n",
      "The second is the SROIE dataset4 for Scanned Receipts Information\n",
      "Extraction. The third is the RVL-CDIP dataset5 [8] for document\n",
      "image classification, which consists of 400,000 grayscale images in\n",
      "16 classes. Experiments illustrate that the pre-trained LayoutLM\n",
      "model significantly outperforms several SOTA pre-trained models\n",
      "on these benchmark datasets, demonstrating the enormous advan-\n",
      "tage for pre-training of text and layout information in document\n",
      "image understanding tasks.\n",
      "The contributions of this paper are summarized as follows:\n",
      "\n",
      "=>\n",
      " • For the first time, textual and layout information from scanned\n",
      "document images is pre-trained in a single framework. Image\n",
      "features are also leveraged to achieve new state-of-the-art\n",
      "results.\n",
      "• LayoutLM uses the masked visual-language model and the\n",
      "multi-label document classification as the training objectives,\n",
      "which significantly outperforms several SOTA pre-trained\n",
      "models in document image understanding tasks.\n",
      "• The code and pre-trained models are publicly available at\n",
      "https://aka.ms/layoutlm for more downstream tasks.\n",
      "\n",
      "=>\n",
      " 2\n",
      "LAYOUTLM\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " In this section, we briefly review the BERT model, and introduce\n",
      "how we extend to jointly model text and layout information in the\n",
      "LayoutLM framework.\n",
      "\n",
      "=>\n",
      " 3https://guillaumejaume.github.io/FUNSD/\n",
      "4https://rrc.cvc.uab.es/?ch=13\n",
      "5https://www.cs.cmu.edu/~aharley/rvl-cdip/\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Text\n",
      "Embeddings\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Position\n",
      "Embeddings (x0)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Position\n",
      "Embeddings (y0)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Position\n",
      "Embeddings (x1)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Position\n",
      "Embeddings (y1)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " E(86)\n",
      "E(117)\n",
      "E(227)\n",
      "E(281)\n",
      "E(303)\n",
      "E(415)\n",
      "E(468)\n",
      "E(556)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " E(138)\n",
      "E(138)\n",
      "E(138)\n",
      "E(138)\n",
      "E(139)\n",
      "E(138)\n",
      "E(139)\n",
      "E(139)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " E(112)\n",
      "E(162)\n",
      "E(277)\n",
      "E(293)\n",
      "E(331)\n",
      "E(464)\n",
      "E(487)\n",
      "E(583)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " E(148)\n",
      "E(148)\n",
      "E(153)\n",
      "E(148)\n",
      "E(149)\n",
      "E(149)\n",
      "E(149)\n",
      "E(150)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " +\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " +\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " +\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " +\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " E(Date)\n",
      "E(Routed:)\n",
      "E(January)\n",
      "E(11,)\n",
      "E(1994)\n",
      "E(Contract)\n",
      "E(No.)\n",
      "E(4011)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " E(589)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " E(139)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " E(621)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " E(150)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " +\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " +\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " +\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " +\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " E(0000)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " E(0)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " E(0)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " E(maxW)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " E(maxH)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " +\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " +\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " +\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " +\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " E([CLS])\n",
      "Faster R-CNN\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " FC Layers\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Pre-trained LayoutLM\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Pre-built\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " OCR/\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " PDF \n",
      "Parser\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " ROI\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Image\n",
      "Embeddings\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Date\n",
      "Routed:\n",
      "January\n",
      "11,\n",
      "1994\n",
      "Contract\n",
      "No.\n",
      "4011\n",
      "0000\n",
      "[CLS]\n",
      "LayoutLM\n",
      "Embeddings\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " +\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Downstream Tasks\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Figure 2: An example of LayoutLM, where 2-D layout and image embeddings are integrated into the original BERT architecture.\n",
      "The LayoutLM embeddings and image embeddings from Faster R-CNN work together for downstream tasks.\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " 2.1\n",
      "The BERT Model\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " The BERT model is an attention-based bidirectional language mod-\n",
      "eling approach. It has been verified that the BERT model shows\n",
      "effective knowledge transfer from the self-supervised task with\n",
      "large-scale training data. The architecture of BERT is basically a\n",
      "multi-layer bidirectional Transformer encoder. It accepts a sequence\n",
      "of tokens and stacks multiple layers to produce final representa-\n",
      "tions. In detail, given a set of tokens processed using WordPiece, the\n",
      "input embeddings are computed by summing the corresponding\n",
      "word embeddings, position embeddings, and segment embeddings.\n",
      "Then, these input embeddings are passed through a multi-layer\n",
      "bidirectional Transformer that can generate contextualized repre-\n",
      "sentations with an adaptive attention mechanism.\n",
      "There are two steps in the BERT framework: pre-training and\n",
      "fine-tuning. During the pre-training, the model uses two objectives\n",
      "to learn the language representation: Masked Language Modeling\n",
      "(MLM) and Next Sentence Prediction (NSP), where MLM randomly\n",
      "masks some input tokens and the objective is to recover these\n",
      "masked tokens, and NSP is a binary classification task taking a\n",
      "pair of sentences as inputs and classifying whether they are two\n",
      "consecutive sentences. In the fine-tuning, task-specific datasets are\n",
      "used to update all parameters in an end-to-end way. The BERT\n",
      "model has been successfully applied in a set of NLP tasks.\n",
      "\n",
      "=>\n",
      " 2.2\n",
      "The LayoutLM Model\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Although BERT-like models become the state-of-the-art techniques\n",
      "on several challenging NLP tasks, they usually leverage text infor-\n",
      "mation only for any kind of inputs. When it comes to visually rich\n",
      "documents, there is much more information that can be encoded\n",
      "into the pre-trained model. Therefore, we propose to utilize the\n",
      "visually rich information from document layouts and align them\n",
      "with the input texts. Basically, there are two types of features which\n",
      "\n",
      "=>\n",
      " substantially improve the language representation in a visually rich\n",
      "document, which are:\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Document Layout Information. It is evident that the relative po-\n",
      "sitions of words in a document contribute a lot to the semantic\n",
      "representation. Taking form understanding as an example, given a\n",
      "key in a form (e.g., “Passport ID:”), its corresponding value is much\n",
      "more likely on its right or below instead of on the left or above.\n",
      "Therefore, we can embed these relative positions information as\n",
      "2-D position representation. Based on the self-attention mechanism\n",
      "within the Transformer, embedding 2-D position features into the\n",
      "language representation will better align the layout information\n",
      "with the semantic representation.\n",
      "\n",
      "=>\n",
      " Visual Information. Compared with the text information, the\n",
      "visual information is another significantly important feature in doc-\n",
      "ument representations. Typically, documents contain some visual\n",
      "signals to show the importance and priority of document segments.\n",
      "The visual information can be represented by image features and ef-\n",
      "fectively utilized in document representations. For document-level\n",
      "visual features, the whole image can indicate the document layout,\n",
      "which is an essential feature for document image classification. For\n",
      "word-level visual features, styles such as bold, underline, and italic,\n",
      "are also significant hints for the sequence labeling tasks. There-\n",
      "fore, we believe that combining the image features with traditional\n",
      "text representations can bring richer semantic representations to\n",
      "documents.\n",
      "\n",
      "=>\n",
      " 2.3\n",
      "Model Architecture\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " To take advantage of existing pre-trained models and adapt to\n",
      "document image understanding tasks, we use the BERT architecture\n",
      "as the backbone and add two new input embeddings: a 2-D position\n",
      "embedding and an image embedding.\n",
      "\n",
      "=>\n",
      " 2-D Position Embedding. Unlike the position embedding that\n",
      "models the word position in a sequence, 2-D position embedding\n",
      "aims to model the relative spatial position in a document. To repre-\n",
      "sent the spatial position of elements in scanned document images,\n",
      "we consider a document page as a coordinate system with the top-\n",
      "left origin. In this setting, the bounding box can be precisely defined\n",
      "by (x0, y0, x1, y1), where (x0, y0) corresponds to the position of the\n",
      "upper left in the bounding box, and (x1, y1) represents the position\n",
      "of the lower right. We add four position embedding layers with two\n",
      "embedding tables, where the embedding layers representing the\n",
      "same dimension share the same embedding table. This means that\n",
      "we look up the position embedding of x0 and x1 in the embedding\n",
      "table X and lookup y0 and y1 in table Y.\n",
      "\n",
      "=>\n",
      " Image Embedding. To utilize the image feature of a document and\n",
      "align the image feature with the text, we add an image embedding\n",
      "layer to represent image features in language representation. In\n",
      "more detail, with the bounding box of each word from OCR results,\n",
      "we split the image into several pieces, and they have a one-to-one\n",
      "correspondence with the words. We generate the image region\n",
      "features with these pieces of images from the Faster R-CNN [19]\n",
      "model as the token image embeddings. For the [CLS] token, we\n",
      "also use the Faster R-CNN model to produce embeddings using the\n",
      "whole scanned document image as the Region of Interest (ROI) to\n",
      "benefit the downstream tasks which need the representation of the\n",
      "[CLS] token.\n",
      "\n",
      "=>\n",
      " 2.4\n",
      "Pre-training LayoutLM\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Task #1: Masked Visual-Language Model. Inspired by the masked\n",
      "language model, we propose the Masked Visual-language Model\n",
      "(MVLM) to learn the language representation with the clues of 2-D\n",
      "position embeddings and text embeddings. During the pre-training,\n",
      "we randomly mask some of the input tokens but keep the corre-\n",
      "sponding 2-D position embeddings, and then the model is trained\n",
      "to predict the masked tokens given the contexts. In this way, the\n",
      "LayoutLM model not only understands the language contexts but\n",
      "also utilizes the corresponding 2-D position information, thereby\n",
      "bridging the gap between the visual and language modalities.\n",
      "\n",
      "=>\n",
      " Task #2: Multi-label Document Classification. For document im-\n",
      "age understanding, many tasks require the model to generate high-\n",
      "quality document-level representations. As the IIT-CDIP Test Col-\n",
      "lection includes multiple tags for each document image, we also\n",
      "use a Multi-label Document Classification (MDC) loss during the\n",
      "pre-training phase. Given a set of scanned documents, we use the\n",
      "document tags to supervise the pre-training process so that the\n",
      "model can cluster the knowledge from different domains and gener-\n",
      "ate better document-level representation. Since the MDC loss needs\n",
      "the label for each document image that may not exist for larger\n",
      "datasets, it is optional during the pre-training and may not be used\n",
      "for pre-training larger models in the future. We will compare the\n",
      "performance of MVLM and MVLM+MDC in Section 3.\n",
      "\n",
      "=>\n",
      " 2.5\n",
      "Fine-tuning LayoutLM\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " The pre-trained LayoutLM model is fine-tuned on three document\n",
      "image understanding tasks, including a form understanding task, a\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " receipt understanding task as well as a document image classifica-\n",
      "tion task. For the form and receipt understanding tasks, LayoutLM\n",
      "predicts {B, I, E, S, O} tags for each token and uses sequential label-\n",
      "ing to detect each type of entity in the dataset. For the document\n",
      "image classification task, LayoutLM predicts the class labels using\n",
      "the representation of the [CLS] token.\n",
      "\n",
      "=>\n",
      " 3\n",
      "EXPERIMENTS\n",
      "3.1\n",
      "Pre-training Dataset\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " The performance of pre-trained models is largely determined by\n",
      "the scale and quality of datasets. Therefore, we need a large-scale\n",
      "scanned document image dataset to pre-train the LayoutLM model.\n",
      "Our model is pre-trained on the IIT-CDIP Test Collection 1.0, which\n",
      "contains more than 6 million documents, with more than 11 million\n",
      "scanned document images. Moreover, each document has its cor-\n",
      "responding text and metadata stored in XML files. The text is the\n",
      "content produced by applying OCR to document images. The meta-\n",
      "data describes the properties of the document, such as the unique\n",
      "identity and document labels. Although the metadata contains er-\n",
      "roneous and inconsistent tags, the scanned document images in\n",
      "this large-scale dataset are perfectly suitable for pre-training our\n",
      "model.\n",
      "\n",
      "=>\n",
      " 3.2\n",
      "Fine-tuning Dataset\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " The FUNSD Dataset. We evaluate our approach on the FUNSD\n",
      "dataset for form understanding in noisy scanned documents. This\n",
      "dataset includes 199 real, fully annotated, scanned forms with 9,707\n",
      "semantic entities and 31,485 words. These forms are organized as a\n",
      "list of semantic entities that are interlinked. Each semantic entity\n",
      "comprises a unique identifier, a label (i.e., question, answer, header,\n",
      "or other), a bounding box, a list of links with other entities, and a\n",
      "list of words. The dataset is split into 149 training samples and 50\n",
      "testing samples. We adopt the word-level F1 score as the evaluation\n",
      "metric.\n",
      "\n",
      "=>\n",
      " The SROIE Dataset. We also evaluate our model on the SROIE\n",
      "dataset for receipt information extraction (Task 3). The dataset\n",
      "contains 626 receipts for training and 347 receipts for testing. Each\n",
      "receipt is organized as a list of text lines with bounding boxes. Each\n",
      "receipt is labeled with four types of entities which are {company,\n",
      "date, address, total}. The evaluation metric is the exact match of the\n",
      "entity recognition results in the F1 score.\n",
      "\n",
      "=>\n",
      " The RVL-CDIP Dataset. The RVL-CDIP dataset consists of 400,000\n",
      "grayscale images in 16 classes, with 25,000 images per class. There\n",
      "are 320,000 training images, 40,000 validation images, and 40,000\n",
      "test images. The images are resized, so their largest dimension does\n",
      "not exceed 1,000 pixels. The 16 classes include {letter, form, email,\n",
      "handwritten, advertisement, scientific report, scientific publication,\n",
      "specification, file folder, news article, budget, invoice, presentation,\n",
      "questionnaire, resume, memo}. The evaluation metric is the overall\n",
      "classification accuracy.\n",
      "\n",
      "=>\n",
      " 3.3\n",
      "Document Pre-processing\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " To utilize the layout information of each document, we need to\n",
      "obtain the location of each token. However, the pre-training dataset\n",
      "(IIT-CDIP Test Collection) only contains pure texts while missing\n",
      "\n",
      "=>\n",
      " their corresponding bounding boxes. In this case, we re-process the\n",
      "scanned document images to obtain the necessary layout informa-\n",
      "tion. Like the original pre-processing in IIT-CDIP Test Collection,\n",
      "we similarly process the dataset by applying OCR to document\n",
      "images. The difference is that we obtain both the recognized words\n",
      "and their corresponding locations in the document image. Thanks\n",
      "to Tesseract6, an open-source OCR engine, we can easily obtain the\n",
      "recognition as well as the 2-D positions. We store the OCR results in\n",
      "hOCR format, a standard specification format which clearly defines\n",
      "the OCR results of one single document image using a hierarchical\n",
      "representation.\n",
      "\n",
      "=>\n",
      " 3.4\n",
      "Model Pre-training\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " We initialize the weight of LayoutLM model with the pre-trained\n",
      "BERT base model. Specifically, our BASE model has the same ar-\n",
      "chitecture: a 12-layer Transformer with 768 hidden sizes, and 12\n",
      "attention heads, which contains about 113M parameters. Therefore,\n",
      "we use the BERT base model to initialize all modules in our model\n",
      "except the 2-D position embedding layer. For the LARGE setting,\n",
      "our model has a 24-layer Transformer with 1,024 hidden sizes and\n",
      "16 attention heads, which is initialized by the pre-trained BERT\n",
      "LARGE model and contains about 343M parameters. Following [4],\n",
      "we select 15% of the input tokens for prediction. We replace these\n",
      "masked tokens with the [MASK] token 80% of the time, a random to-\n",
      "ken 10% of the time, and an unchanged token 10% of the time. Then,\n",
      "the model predicts the corresponding token with the cross-entropy\n",
      "loss.\n",
      "In addition, we also add the 2-D position embedding layers with\n",
      "four embedding representations (x0, y0, x1, y1), where (x0, y0) cor-\n",
      "responds to the position of the upper left in the bounding box, and\n",
      "(x1, y1) represents the position of the lower right. Considering that\n",
      "the document layout may vary in different page size, we scale the\n",
      "actual coordinate to a “virtual” coordinate: the actual coordinate is\n",
      "scaled to have a value from 0 to 1,000. Furthermore, we also use the\n",
      "ResNet-101 model as the backbone network in the Faster R-CNN\n",
      "model, which is pre-trained on the Visual Genome dataset [12].\n",
      "We train our model on 8 NVIDIA Tesla V100 32GB GPUs with a\n",
      "total batch size of 80. The Adam optimizer is used with an initial\n",
      "learning rate of 5e-5 and a linear decay learning rate schedule. The\n",
      "BASE model takes 80 hours to finish one epoch on 11M documents,\n",
      "while the LARGE model takes nearly 170 hours to finish one epoch.\n",
      "\n",
      "=>\n",
      " 3.5\n",
      "Task-specific Fine-tuning\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " We evaluate the LayoutLM model on three document image under-\n",
      "standing tasks: Form Understanding, Receipt Understanding,\n",
      "and Document Image Classification. We follow the typical fine-\n",
      "tuning strategy and update all parameters in an end-to-end way on\n",
      "task-specific datasets.\n",
      "\n",
      "=>\n",
      " Form Understanding. This task requires extracting and structur-\n",
      "ing the textual content of forms. It aims to extract key-value pairs\n",
      "from the scanned form images. In more detail, this task includes\n",
      "two sub-tasks: semantic labeling and semantic linking. Semantic\n",
      "labeling is the task of aggregating words as semantic entities and\n",
      "assigning pre-defined labels to them. Semantic linking is the task\n",
      "\n",
      "=>\n",
      " 6https://github.com/tesseract-ocr/tesseract\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " of predicting the relations between semantic entities. In this work,\n",
      "we focus on the semantic labeling task, while semantic linking\n",
      "is out of the scope. To fine-tune LayoutLM on this task, we treat\n",
      "semantic labeling as a sequence labeling problem. We pass the final\n",
      "representation into a linear layer followed by a softmax layer to\n",
      "predict the label of each token. The model is trained for 100 epochs\n",
      "with a batch size of 16 and a learning rate of 5e-5.\n",
      "\n",
      "=>\n",
      " Receipt Understanding. This task requires filling several pre-\n",
      "defined semantic slots according to the scanned receipt images.\n",
      "For instance, given a set of receipts, we need to fill specific slots (\n",
      "i.g., company, address, date, and total). Different from the form un-\n",
      "derstanding task that requires labeling all matched entities and key-\n",
      "value pairs, the number of semantic slots is fixed with pre-defined\n",
      "keys. Therefore, the model only needs to predict the corresponding\n",
      "values using the sequence labeling method.\n",
      "\n",
      "=>\n",
      " Document Image Classification. Given a visually rich document,\n",
      "this task aims to predict the corresponding category for each doc-\n",
      "ument image. Distinct from the existing image-based approaches,\n",
      "our model includes not only image representations but also text and\n",
      "layout information using the multimodal architecture in LayoutLM.\n",
      "Therefore, our model can combine the text, layout, and image in-\n",
      "formation in a more effective way. To fine-tune our model on this\n",
      "task, we concatenate the output from the LayoutLM model and the\n",
      "whole image embedding, followed by a softmax layer for category\n",
      "prediction. We fine-tune the model for 30 epochs with a batch size\n",
      "of 40 and a learning rate of 2e-5.\n",
      "\n",
      "=>\n",
      " 3.6\n",
      "Results\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Form Understanding. We evaluate the form understanding task\n",
      "on the FUNSD dataset. The experiment results are shown in Table 1.\n",
      "We compare the LayoutLM model with two SOTA pre-trained NLP\n",
      "models: BERT and RoBERTa [16]. The BERT BASE model achieves\n",
      "0.603 and while the LARGE model achieves 0.656 in F1. Compared\n",
      "to BERT, the RoBERTa performs much better on this dataset as it is\n",
      "trained using larger data with more epochs. Due to the time limita-\n",
      "tion, we present 4 settings for LayoutLM, which are 500K document\n",
      "pages with 6 epochs, 1M with 6 epochs, 2M with 6 epochs as well\n",
      "as 11M with 2 epochs. It is observed that the LayoutLM model sub-\n",
      "stantially outperforms existing SOTA pre-training baselines. With\n",
      "the BASE architecture, the LayoutLM model with 11M training\n",
      "data achieves 0.7866 in F1, which is much higher than BERT and\n",
      "RoBERTa with the similar size of parameters. In addition, we also\n",
      "add the MDC loss in the pre-training step and it does bring substan-\n",
      "tial improvements on the FUNSD dataset. Finally, the LayoutLM\n",
      "model achieves the best performance of 0.7927 when using the text,\n",
      "layout, and image information at the same time.\n",
      "In addition, we also evaluate the LayoutLM model with different\n",
      "data and epochs on the FUNSD dataset, which is shown in Table 2.\n",
      "For different data settings, we can see that the overall accuracy\n",
      "is monotonically increased as more epochs are trained during the\n",
      "pre-training step. Furthermore, the accuracy is also improved as\n",
      "more data is fed into the LayoutLM model. As the FUNSD dataset\n",
      "contains only 149 images for fine-tuning, the results confirm that\n",
      "the pre-training of text and layout is effective for scanned document\n",
      "understanding especially with low resource settings.\n",
      "\n",
      "=>\n",
      " Modality\n",
      "Model\n",
      "Precision\n",
      "Recall\n",
      "F1\n",
      "#Parameters\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Text only\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " BERTBASE\n",
      "0.5469\n",
      "0.671\n",
      "0.6026\n",
      "110M\n",
      "RoBERTaBASE\n",
      "0.6349\n",
      "0.6975\n",
      "0.6648\n",
      "125M\n",
      "BERTLARGE\n",
      "0.6113\n",
      "0.7085\n",
      "0.6563\n",
      "340M\n",
      "RoBERTaLARGE\n",
      "0.678\n",
      "0.7391\n",
      "0.7072\n",
      "355M\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Text + Layout\n",
      "MVLM\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " LayoutLMBASE (500K, 6 epochs)\n",
      "0.665\n",
      "0.7355\n",
      "0.6985\n",
      "113M\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.6909\n",
      "0.7735\n",
      "0.7299\n",
      "113M\n",
      "LayoutLMBASE (2M, 6 epochs)\n",
      "0.7377\n",
      "0.782\n",
      "0.7592\n",
      "113M\n",
      "LayoutLMBASE (11M, 2 epochs)\n",
      "0.7597\n",
      "0.8155\n",
      "0.7866\n",
      "113M\n",
      "\n",
      "=>\n",
      " Text + Layout\n",
      "MVLM+MDC\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.7076\n",
      "0.7695\n",
      "0.7372\n",
      "113M\n",
      "LayoutLMBASE (11M, 1 epoch)\n",
      "0.7194\n",
      "0.7780\n",
      "0.7475\n",
      "113M\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Text + Layout\n",
      "MVLM\n",
      "LayoutLMLARGE (1M, 6 epochs)\n",
      "0.7171\n",
      "0.805\n",
      "0.7585\n",
      "343M\n",
      "LayoutLMLARGE (11M, 1 epoch)\n",
      "0.7536\n",
      "0.806\n",
      "0.7789\n",
      "343M\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Text + Layout + Image\n",
      "MVLM\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.7101\n",
      "0.7815\n",
      "0.7441\n",
      "160M\n",
      "LayoutLMBASE (11M, 2 epochs)\n",
      "0.7677\n",
      "0.8195\n",
      "0.7927\n",
      "160M\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Table 1: Model accuracy (Precision, Recall, F1) on the FUNSD dataset\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " # Pre-training Data\n",
      "# Pre-training Epochs\n",
      "Precision\n",
      "Recall\n",
      "F1\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " 500K\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " 1 epoch\n",
      "0.5779\n",
      "0.6955\n",
      "0.6313\n",
      "2 epochs\n",
      "0.6217\n",
      "0.705\n",
      "0.6607\n",
      "3 epochs\n",
      "0.6304\n",
      "0.718\n",
      "0.6713\n",
      "4 epochs\n",
      "0.6383\n",
      "0.7175\n",
      "0.6756\n",
      "5 epochs\n",
      "0.6568\n",
      "0.734\n",
      "0.6933\n",
      "6 epochs\n",
      "0.665\n",
      "0.7355\n",
      "0.6985\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " 1M\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " 1 epoch\n",
      "0.6156\n",
      "0.7005\n",
      "0.6552\n",
      "2 epochs\n",
      "0.6545\n",
      "0.737\n",
      "0.6933\n",
      "3 epochs\n",
      "0.6794\n",
      "0.762\n",
      "0.7184\n",
      "4 epochs\n",
      "0.6812\n",
      "0.766\n",
      "0.7211\n",
      "5 epochs\n",
      "0.6863\n",
      "0.7625\n",
      "0.7224\n",
      "6 epochs\n",
      "0.6909\n",
      "0.7735\n",
      "0.7299\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " 2M\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " 1 epoch\n",
      "0.6599\n",
      "0.7355\n",
      "0.6957\n",
      "2 epochs\n",
      "0.6938\n",
      "0.759\n",
      "0.7249\n",
      "3 epochs\n",
      "0.6915\n",
      "0.7655\n",
      "0.7266\n",
      "4 epochs\n",
      "0.7081\n",
      "0.781\n",
      "0.7427\n",
      "5 epochs\n",
      "0.7228\n",
      "0.7875\n",
      "0.7538\n",
      "6 epochs\n",
      "0.7377\n",
      "0.782\n",
      "0.7592\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " 11M\n",
      "1 epoch\n",
      "0.7464\n",
      "0.7815\n",
      "0.7636\n",
      "2 epochs\n",
      "0.7597\n",
      "0.8155\n",
      "0.7866\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Table 2: LayoutLMBASE (Text + Layout, MVLM) accuracy with different data and epochs on the FUNSD dataset\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Furthermore, we compare different initialization methods for\n",
      "the LayoutLM model including from scratch, BERT and RoBERTa.\n",
      "The results in Table 3 show that the LayoutLMBASE model initial-\n",
      "ized with RoBERTaBASE outperforms BERTBASE by 2.1 points in F1.\n",
      "For the LARGE setting, the LayoutLMLARGE model initialized with\n",
      "RoBERTaLARGE further improve 1.3 points over the BERTLARGE\n",
      "model. We will pre-train more models with RoBERTa as the initial-\n",
      "ization in the future, especially for the LARGE settings.\n",
      "\n",
      "=>\n",
      " Receipt Understanding. We evaluate the receipt understanding\n",
      "task using the SROIE dataset. The results are shown in Table 4. As\n",
      "we only test the performance of the Key Information Extraction\n",
      "task in SROIE, we would like to eliminate the effect of incorrect\n",
      "OCR results. Therefore, we pre-process the training data by using\n",
      "the ground truth OCR and run a set of experiments using the base-\n",
      "line models (BERT & RoBERTa) as well as the LayoutLM model.\n",
      "The results show that the LayoutLMLARGE model trained with 11M\n",
      "\n",
      "=>\n",
      " Initialization\n",
      "Model\n",
      "Precision\n",
      "Recall\n",
      "F1\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " SCRATCH\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.5630\n",
      "0.6728\n",
      "0.6130\n",
      "BERTBASE\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.6909\n",
      "0.7735\n",
      "0.7299\n",
      "RoBERTaBASE\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.7173\n",
      "0.7888\n",
      "0.7514\n",
      "SCRATCH\n",
      "LayoutLMLARGE (11M, 1 epoch)\n",
      "0.6845\n",
      "0.7804\n",
      "0.7293\n",
      "BERTLARGE\n",
      "LayoutLMLARGE (11M, 1 epoch)\n",
      "0.7536\n",
      "0.8060\n",
      "0.7789\n",
      "RoBERTaLARGE\n",
      "LayoutLMLARGE (11M, 1 epoch)\n",
      "0.7681\n",
      "0.8188\n",
      "0.7926\n",
      "\n",
      "=>\n",
      " Table 3: Different initialization methods for BASE and LARGE (Text + Layout, MVLM)\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " document images achieve an F1 score of 0.9524, which is signifi-\n",
      "cantly better than the first place in the competition leaderboard.\n",
      "This result also verifies that the pre-trained LayoutLM not only per-\n",
      "forms well on the in-domain dataset (FUNSD) but also outperforms\n",
      "several strong baselines on the out-of-domain dataset like SROIE.\n",
      "\n",
      "=>\n",
      " Document Image Classification. Finally, we evaluate the docu-\n",
      "ment image classification task using the RVL-CDIP dataset. Doc-\n",
      "ument images are different from other natural images as most of\n",
      "the content in document images are texts in a variety of styles and\n",
      "layouts. Traditionally, image-based classification models with pre-\n",
      "training perform much better than the text-based models, which\n",
      "is shown in Table 5. We can see that either BERT or RoBERTa\n",
      "underperforms the image-based approaches, illustrating that text\n",
      "information is not sufficient for this task, and it still needs layout\n",
      "and image features. We address this issue by using the LayoutLM\n",
      "model for this task. Results show that, even without the image\n",
      "features, LayoutLM still outperforms the single model of the image-\n",
      "based approaches. After integrating the image embeddings, the\n",
      "LayoutLM achieves the accuracy of 94.42%, which is significantly\n",
      "better than several SOTA baselines for document image classifi-\n",
      "cation. It is observed that our model performs best in the \"email\"\n",
      "category while performs worst in the \"form\" category. We will\n",
      "further investigate how to take advantage of both pre-trained Lay-\n",
      "outLM and image models, as well as involve image information in\n",
      "the pre-training step for the LayoutLM model.\n",
      "\n",
      "=>\n",
      " 4\n",
      "RELATED WORK\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " The research of Document Analysis and Recognition (DAR) dates\n",
      "to the early 1990s. The mainstream approaches can be divided\n",
      "into three categories: rule-based approaches, conventional machine\n",
      "learning approaches, and deep learning approaches.\n",
      "\n",
      "=>\n",
      " 4.1\n",
      "Rule-based Approaches\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " The rule-based approaches [6, 13, 18, 23] contain two types of anal-\n",
      "ysis methods: bottom-up and top-down. Bottom-up methods [5,\n",
      "13, 23] usually detect the connected components of black pixels as\n",
      "the basic computational units in document images, and the docu-\n",
      "ment segmentation process is to combine them into higher-level\n",
      "structures through different heuristics and label them according\n",
      "to different structural features. Docstrum algorithm [18] is among\n",
      "the earliest successful bottom-up algorithms that are based on the\n",
      "connected component analysis. It groups connected components\n",
      "on a polar structure to derive the final segmentation. [23] use a\n",
      "special distance-metric between different components to construct\n",
      "\n",
      "=>\n",
      " a physical page structure. They further reduced the time complexity\n",
      "by using heuristics and path compression algorithms.\n",
      "The top-down methods often recursively split a page into columns,\n",
      "blocks, text lines, and tokens. [6] propose replacing the basic unit\n",
      "with the black pixels from all the pixels, and the method decom-\n",
      "posed the document using the recursive the X-Y cut algorithm to\n",
      "establish an X-Y tree, which makes complex documents decompose\n",
      "more easily. Although these methods perform well on some doc-\n",
      "uments, they require extensive human efforts to figure out better\n",
      "rules, while sometimes failing to generalize to documents from\n",
      "other sources. Therefore, it is inevitable to leverage machine learn-\n",
      "ing approaches in the DAR research.\n",
      "\n",
      "=>\n",
      " 4.2\n",
      "Machine Learning Approaches\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " With the development of conventional machine learning, statistical\n",
      "machine learning approaches [17, 22] have become the mainstream\n",
      "for document segmentation tasks during the past decade. [22] con-\n",
      "sider the layout information of a document as a parsing problem,\n",
      "and globally search the optimal parsing tree based on a grammar-\n",
      "based loss function. They utilize a machine learning approach to\n",
      "select features and train all parameters during the parsing process.\n",
      "Meanwhile, artificial neural networks [17] have been extensively\n",
      "applied to document analysis and recognition. Most efforts have\n",
      "been devoted to the recognition of isolated handwritten and printed\n",
      "characters with widely recognized successful results. In addition to\n",
      "the ANN model, SVM and GMM [27] have been used in document\n",
      "layout analysis tasks. For machine learning approaches, they are\n",
      "usually time-consuming to design manually crafted features and\n",
      "difficult to obtain a highly abstract semantic context. In addition,\n",
      "these methods usually relied on visual cues but ignored textual\n",
      "information.\n",
      "\n",
      "=>\n",
      " 4.3\n",
      "Deep Learning Approaches\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Recently, deep learning methods have become the mainstream and\n",
      "de facto standard for many machine learning problems. Theoreti-\n",
      "cally, they can fit any arbitrary functions through the stacking of\n",
      "multi-layer neural networks and have been verified to be effective\n",
      "in many research areas. [28] treat the document semantic structure\n",
      "extraction task as a pixel-by-pixel classification problem. They pro-\n",
      "pose a multimodal neural network that considers visual and textual\n",
      "information, while the limitation of this work is that they only\n",
      "used the network to assist heuristic algorithms to classify candidate\n",
      "bounding boxes rather than an end-to-end approach. [26] propose a\n",
      "lightweight model of document layout analysis for mobile and cloud\n",
      "services. The model uses one-dimensional information of images for\n",
      "\n",
      "=>\n",
      " Modality\n",
      "Model\n",
      "Precision\n",
      "Recall\n",
      "F1\n",
      "#Parameters\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Text only\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " BERTBASE\n",
      "0.9099\n",
      "0.9099\n",
      "0.9099\n",
      "110M\n",
      "RoBERTaBASE\n",
      "0.9107\n",
      "0.9107\n",
      "0.9107\n",
      "125M\n",
      "BERTLARGE\n",
      "0.9200\n",
      "0.9200\n",
      "0.9200\n",
      "340M\n",
      "RoBERTaLARGE\n",
      "0.9280\n",
      "0.9280\n",
      "0.9280\n",
      "355M\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Text + Layout\n",
      "MVLM\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " LayoutLMBASE (500K, 6 epochs)\n",
      "0.9388\n",
      "0.9388\n",
      "0.9388\n",
      "113M\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.9380\n",
      "0.9380\n",
      "0.9380\n",
      "113M\n",
      "LayoutLMBASE (2M, 6 epochs)\n",
      "0.9431\n",
      "0.9431\n",
      "0.9431\n",
      "113M\n",
      "LayoutLMBASE (11M, 2 epochs)\n",
      "0.9438\n",
      "0.9438\n",
      "0.9438\n",
      "113M\n",
      "\n",
      "=>\n",
      " Text + Layout\n",
      "MVLM+MDC\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.9402\n",
      "0.9402\n",
      "0.9402\n",
      "113M\n",
      "LayoutLMBASE (11M, 1 epoch)\n",
      "0.9460\n",
      "0.9460\n",
      "0.9460\n",
      "113M\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Text + Layout\n",
      "MVLM\n",
      "LayoutLMLARGE (1M, 6 epochs)\n",
      "0.9416\n",
      "0.9416\n",
      "0.9416\n",
      "343M\n",
      "LayoutLMLARGE (11M, 1 epoch)\n",
      "0.9524\n",
      "0.9524\n",
      "0.9524\n",
      "343M\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Text + Layout + Image\n",
      "MVLM\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.9416\n",
      "0.9416\n",
      "0.9416\n",
      "160M\n",
      "LayoutLMBASE (11M, 2 epochs)\n",
      "0.9467\n",
      "0.9467\n",
      "0.9467\n",
      "160M\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Baseline\n",
      "Ranking 1st in SROIE\n",
      "0.9402\n",
      "0.9402\n",
      "0.9402\n",
      "-\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Table 4: Model accuracy (Precision, Recall, F1) on the SROIE dataset\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Modality\n",
      "Model\n",
      "Accuracy\n",
      "#Parameters\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Text only\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " BERTBASE\n",
      "89.81%\n",
      "110M\n",
      "RoBERTaBASE\n",
      "90.06%\n",
      "125M\n",
      "BERTLARGE\n",
      "89.92%\n",
      "340M\n",
      "RoBERTaLARGE\n",
      "90.11%\n",
      "355M\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Text + Layout\n",
      "MVLM\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " LayoutLMBASE (500K, 6 epochs)\n",
      "91.25%\n",
      "113M\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "91.48%\n",
      "113M\n",
      "LayoutLMBASE (2M, 6 epochs)\n",
      "91.65%\n",
      "113M\n",
      "LayoutLMBASE (11M, 2 epochs)\n",
      "91.78%\n",
      "113M\n",
      "\n",
      "=>\n",
      " Text + Layout\n",
      "MVLM+MDC\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "91.74%\n",
      "113M\n",
      "LayoutLMBASE (11M, 1 epoch)\n",
      "91.78%\n",
      "113M\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Text + Layout\n",
      "MVLM\n",
      "LayoutLMLARGE (1M, 6 epochs)\n",
      "91.88%\n",
      "343M\n",
      "LayoutLMLARGE (11M, 1 epoch)\n",
      "91.90%\n",
      "343M\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Text + Layout + Image\n",
      "MVLM\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "94.31%\n",
      "160M\n",
      "LayoutLMBASE (11M, 2 epochs)\n",
      "94.42%\n",
      "160M\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " Baselines\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " VGG-16 [1]\n",
      "90.97%\n",
      "-\n",
      "Stacked CNN Single [2]\n",
      "91.11%\n",
      "-\n",
      "Stacked CNN Ensemble [2]\n",
      "92.21%\n",
      "-\n",
      "InceptionResNetV2 [25]\n",
      "92.63%\n",
      "-\n",
      "LadderNet [20]\n",
      "92.77%\n",
      "-\n",
      "Multimodal Single [3]\n",
      "93.03%\n",
      "-\n",
      "Multimodal Ensemble [3]\n",
      "93.07%\n",
      "-\n",
      "\n",
      "=>\n",
      " Table 5: Classification accuracy on the RVL-CDIP dataset\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " inference and compares it with the model using two-dimensional in-\n",
      "formation, achieving comparable accuracy in the experiments. [11]\n",
      "make use of a fully convolutional encoder-decoder network that\n",
      "predicts a segmentation mask and bounding boxes, and the model\n",
      "significantly outperforms approaches based on sequential text or\n",
      "document images. [24] incorporate contextual information into the\n",
      "\n",
      "=>\n",
      " Faster R-CNN model that involves the inherently localized nature\n",
      "of article contents to improve region detection performance.\n",
      "Existing deep learning approaches for DAR usually confront\n",
      "two limitations: (1) The models often rely on limited labeled data\n",
      "while leaving a large amount of unlabeled data unused. (2) Current\n",
      "deep learning models usually leverage pre-trained CV models or\n",
      "\n",
      "=>\n",
      " NLP models, but do not consider the joint pre-training of text and\n",
      "layout. LayoutLM addresses these two limitations and achieves\n",
      "much better performance compared with the previous baselines.\n",
      "\n",
      "=>\n",
      " 5\n",
      "CONCLUSION AND FUTURE WORK\n",
      "\n",
      "********* skipped\n",
      "=>\n",
      " We present LayoutLM, a simple yet effective pre-training technique\n",
      "with text and layout information in a single framework. Based on\n",
      "the Transformer architecture as the backbone, LayoutLM takes\n",
      "advantage of multimodal inputs, including token embeddings, lay-\n",
      "out embeddings, and image embeddings. Meanwhile, the model\n",
      "can be easily trained in a self-supervised way based on large scale\n",
      "unlabeled scanned document images. We evaluate the LayoutLM\n",
      "model on three tasks: form understanding, receipt understanding,\n",
      "and scanned document image classification. Experiments show\n",
      "that LayoutLM substantially outperforms several SOTA pre-trained\n",
      "models in these tasks.\n",
      "For future research, we will investigate pre-training models with\n",
      "more data and more computation resources. In addition, we will\n",
      "also train LayoutLM using the LARGE architecture with text and\n",
      "layout, as well as involving image embeddings in the pre-training\n",
      "step. Furthermore, we will explore new network architectures and\n",
      "other self-supervised training objectives that may further unlock\n",
      "the power of LayoutLM.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def get_page_span(blocks):\n",
    "    abstract_idx = None\n",
    "    for i in range(len(blocks)):\n",
    "        if re.search(\"Abstract\", blocks[i], re.IGNORECASE):\n",
    "            abstract_idx = i\n",
    "            break\n",
    "    \n",
    "    references_idx = None\n",
    "    for i in range(len(blocks)-1, -1, -1):\n",
    "        if re.search(\"References\", blocks[i], re.IGNORECASE):\n",
    "            references_idx = i\n",
    "            break\n",
    "    \n",
    "    return abstract_idx, references_idx\n",
    "            \n",
    "def clean_block(block: str) -> str:\n",
    "    if re.search(\"^figure\", block, re.IGNORECASE):\n",
    "        return \"\"\n",
    "    \n",
    "    MIN_BLOCK_LENGTH = 20 #words\n",
    "    single_line_block = block.replace(\"\\n\", \" \")\n",
    "    single_line_block = re.sub(r\"-?\\d+(?:\\.\\d+)?\", \" \", single_line_block)\n",
    "    if len(single_line_block.split()) < MIN_BLOCK_LENGTH:\n",
    "        return \"\"\n",
    "\n",
    "    return block\n",
    "\n",
    "def clean_blocks(blocks: list[str]):\n",
    "    abstract_idx, references_idx = get_page_span(blocks)\n",
    "    print(abstract_idx, references_idx)\n",
    "\n",
    "    assert abstract_idx is not None and references_idx is not None\n",
    "    \n",
    "    blocks = blocks[abstract_idx+1: references_idx] \n",
    "    cleaned_blocks = []\n",
    "    for block in blocks:\n",
    "        print(\"=>\\n\", block)\n",
    "        block = clean_block(block)\n",
    "        if block:\n",
    "            cleaned_blocks.append(block)\n",
    "        else:\n",
    "            print(\"********* skipped\")\n",
    "    \n",
    "    return cleaned_blocks\n",
    "\n",
    "cleaned_blocks = clean_blocks(doc_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>\n",
      "Pre-training techniques have been verified successfully in a vari-\n",
      "ety of NLP tasks in recent years. Despite the widespread use of\n",
      "pre-training models for NLP applications, they almost exclusively\n",
      "focus on text-level manipulation, while neglecting layout and style\n",
      "information that is vital for document image understanding. In\n",
      "this paper, we propose the LayoutLM to jointly model interactions\n",
      "between text and layout information across scanned document\n",
      "images, which is beneficial for a great number of real-world doc-\n",
      "ument image understanding tasks such as information extraction\n",
      "from scanned documents. Furthermore, we also leverage image\n",
      "features to incorporate words’ visual information into LayoutLM.\n",
      "To the best of our knowledge, this is the first time that text and\n",
      "layout are jointly learned in a single framework for document-\n",
      "level pre-training. It achieves new state-of-the-art results in several\n",
      "downstream tasks, including form understanding (from 70.72 to\n",
      "79.27), receipt understanding (from 94.02 to 95.24) and document\n",
      "image classification (from 93.07 to 94.42). The code and pre-trained\n",
      "LayoutLM models are publicly available at https://aka.ms/layoutlm.\n",
      "\n",
      "==>\n",
      "ACM Reference Format:\n",
      "Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming\n",
      "Zhou. 2020. LayoutLM: Pre-training of Text and Layout for Document\n",
      "Image Understanding. In Proceedings of the 26th ACM SIGKDD Conference\n",
      "on Knowledge Discovery and Data Mining (KDD ’20), August 23–27, 2020,\n",
      "Virtual Event, CA, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/\n",
      "10.1145/3394486.3403172\n",
      "\n",
      "==>\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior specific permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "KDD ’20, August 23–27, 2020, Virtual Event, CA, USA\n",
      "© 2020 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-7998-4/20/08...$15.00\n",
      "https://doi.org/10.1145/3394486.3403172\n",
      "\n",
      "==>\n",
      "Document AI, or Document Intelligence1, is a relatively new re-\n",
      "search topic that refers techniques for automatically reading, under-\n",
      "standing, and analyzing business documents. Business documents\n",
      "are files that provide details related to a company’s internal and\n",
      "external transactions, which are shown in Figure 1. They may be\n",
      "digital-born, occurring as electronic files, or they may be in scanned\n",
      "form that comes from written or printed on paper. Some common\n",
      "examples of business documents include purchase orders, financial\n",
      "reports, business emails, sales agreements, vendor contracts, letters,\n",
      "invoices, receipts, resumes, and many others. Business documents\n",
      "are critical to a company’s efficiency and productivity. The exact\n",
      "format of a business document may vary, but the information is\n",
      "usually presented in natural language and can be organized in a\n",
      "variety of ways from plain text, multi-column layouts, and a wide\n",
      "variety of tables/forms/figures. Understanding business documents\n",
      "is a very challenging task due to the diversity of layouts and formats,\n",
      "poor quality of scanned document images as well as the complexity\n",
      "of template structures.\n",
      "Nowadays, many companies extract data from business docu-\n",
      "ments through manual efforts that are time-consuming and expen-\n",
      "sive, meanwhile requiring manual customization or configuration.\n",
      "Rules and workflows for each type of document often need to be\n",
      "hard-coded and updated with changes to the specific format or\n",
      "when dealing with multiple formats. To address these problems,\n",
      "document AI models and algorithms are designed to automatically\n",
      "classify, extract, and structuralize information from business doc-\n",
      "uments, accelerating automated document processing workflows.\n",
      "Contemporary approaches for document AI are usually built upon\n",
      "deep neural networks from a computer vision perspective or a natu-\n",
      "ral language processing perspective, or a combination of them. Early\n",
      "attempts usually focused on detecting and analyzing certain parts\n",
      "of a document, such as tabular areas. [7] were the first to propose a\n",
      "table detection method for PDF documents based on Convolutional\n",
      "Neural Networks (CNN). After that, [21, 24, 29] also leveraged more\n",
      "advanced Faster R-CNN model [19] or Mask R-CNN model [9] to\n",
      "further improve the accuracy of document layout analysis. In addi-\n",
      "tion, [28] presented an end-to-end, multimodal, fully convolutional\n",
      "network for extracting semantic structures from document images,\n",
      "taking advantage of text embeddings from pre-trained NLP models.\n",
      "More recently, [15] introduced a Graph Convolutional Networks\n",
      "(GCN) based model to combine textual and visual information for\n",
      "\n",
      "==>\n",
      "information extraction from business documents. Although these\n",
      "models have made significant progress in the document AI area\n",
      "with deep neural networks, most of these methods confront two\n",
      "limitations: (1) They rely on a few human-labeled training samples\n",
      "without fully exploring the possibility of using large-scale unla-\n",
      "beled training samples. (2) They usually leverage either pre-trained\n",
      "CV models or NLP models, but do not consider a joint training of\n",
      "textual and layout information. Therefore, it is important to inves-\n",
      "tigate how self-supervised pre-training of text and layout may help\n",
      "in the document AI area.\n",
      "To this end, we propose LayoutLM, a simple yet effective pre-\n",
      "training method of text and layout for document image understand-\n",
      "ing tasks. Inspired by the BERT model [4], where input textual\n",
      "information is mainly represented by text embeddings and position\n",
      "embeddings, LayoutLM further adds two types of input embeddings:\n",
      "(1) a 2-D position embedding that denotes the relative position of\n",
      "a token within a document; (2) an image embedding for scanned\n",
      "token images within a document. The architecture of LayoutLM is\n",
      "shown in Figure 2. We add these two input embeddings because\n",
      "the 2-D position embedding can capture the relationship among\n",
      "tokens within a document, meanwhile the image embedding can\n",
      "capture some appearance features such as font directions, types,\n",
      "and colors. In addition, we adopt a multi-task learning objective for\n",
      "LayoutLM, including a Masked Visual-Language Model (MVLM)\n",
      "loss and a Multi-label Document Classification (MDC) loss, which\n",
      "further enforces joint pre-training for text and layout. In this work,\n",
      "our focus is the document pre-training based on scanned docu-\n",
      "ment images, while digital-born documents are less challenging\n",
      "because they can be considered as a special case where OCR is\n",
      "not required, thus they are out of the scope of this paper. Specifi-\n",
      "cally, the LayoutLM is pre-trained on the IIT-CDIP Test Collection\n",
      "1.02 [14], which contains more than 6 million scanned documents\n",
      "with 11 million scanned document images. The scanned documents\n",
      "are in a variety of categories, including letter, memo, email, file-\n",
      "folder, form, handwritten, invoice, advertisement, budget, news\n",
      "\n",
      "==>\n",
      "articles, presentation, scientific publication, questionnaire, resume,\n",
      "scientific report, specification, and many others, which is ideal for\n",
      "large-scale self-supervised pre-training. We select three benchmark\n",
      "datasets as the downstream tasks to evaluate the performance of the\n",
      "pre-trained LayoutLM model. The first is the FUNSD dataset3 [10]\n",
      "that is used for spatial layout analysis and form understanding.\n",
      "The second is the SROIE dataset4 for Scanned Receipts Information\n",
      "Extraction. The third is the RVL-CDIP dataset5 [8] for document\n",
      "image classification, which consists of 400,000 grayscale images in\n",
      "16 classes. Experiments illustrate that the pre-trained LayoutLM\n",
      "model significantly outperforms several SOTA pre-trained models\n",
      "on these benchmark datasets, demonstrating the enormous advan-\n",
      "tage for pre-training of text and layout information in document\n",
      "image understanding tasks.\n",
      "The contributions of this paper are summarized as follows:\n",
      "\n",
      "==>\n",
      "• For the first time, textual and layout information from scanned\n",
      "document images is pre-trained in a single framework. Image\n",
      "features are also leveraged to achieve new state-of-the-art\n",
      "results.\n",
      "• LayoutLM uses the masked visual-language model and the\n",
      "multi-label document classification as the training objectives,\n",
      "which significantly outperforms several SOTA pre-trained\n",
      "models in document image understanding tasks.\n",
      "• The code and pre-trained models are publicly available at\n",
      "https://aka.ms/layoutlm for more downstream tasks.\n",
      "\n",
      "==>\n",
      "In this section, we briefly review the BERT model, and introduce\n",
      "how we extend to jointly model text and layout information in the\n",
      "LayoutLM framework.\n",
      "\n",
      "==>\n",
      "The BERT model is an attention-based bidirectional language mod-\n",
      "eling approach. It has been verified that the BERT model shows\n",
      "effective knowledge transfer from the self-supervised task with\n",
      "large-scale training data. The architecture of BERT is basically a\n",
      "multi-layer bidirectional Transformer encoder. It accepts a sequence\n",
      "of tokens and stacks multiple layers to produce final representa-\n",
      "tions. In detail, given a set of tokens processed using WordPiece, the\n",
      "input embeddings are computed by summing the corresponding\n",
      "word embeddings, position embeddings, and segment embeddings.\n",
      "Then, these input embeddings are passed through a multi-layer\n",
      "bidirectional Transformer that can generate contextualized repre-\n",
      "sentations with an adaptive attention mechanism.\n",
      "There are two steps in the BERT framework: pre-training and\n",
      "fine-tuning. During the pre-training, the model uses two objectives\n",
      "to learn the language representation: Masked Language Modeling\n",
      "(MLM) and Next Sentence Prediction (NSP), where MLM randomly\n",
      "masks some input tokens and the objective is to recover these\n",
      "masked tokens, and NSP is a binary classification task taking a\n",
      "pair of sentences as inputs and classifying whether they are two\n",
      "consecutive sentences. In the fine-tuning, task-specific datasets are\n",
      "used to update all parameters in an end-to-end way. The BERT\n",
      "model has been successfully applied in a set of NLP tasks.\n",
      "\n",
      "==>\n",
      "Although BERT-like models become the state-of-the-art techniques\n",
      "on several challenging NLP tasks, they usually leverage text infor-\n",
      "mation only for any kind of inputs. When it comes to visually rich\n",
      "documents, there is much more information that can be encoded\n",
      "into the pre-trained model. Therefore, we propose to utilize the\n",
      "visually rich information from document layouts and align them\n",
      "with the input texts. Basically, there are two types of features which\n",
      "\n",
      "==>\n",
      "Document Layout Information. It is evident that the relative po-\n",
      "sitions of words in a document contribute a lot to the semantic\n",
      "representation. Taking form understanding as an example, given a\n",
      "key in a form (e.g., “Passport ID:”), its corresponding value is much\n",
      "more likely on its right or below instead of on the left or above.\n",
      "Therefore, we can embed these relative positions information as\n",
      "2-D position representation. Based on the self-attention mechanism\n",
      "within the Transformer, embedding 2-D position features into the\n",
      "language representation will better align the layout information\n",
      "with the semantic representation.\n",
      "\n",
      "==>\n",
      "Visual Information. Compared with the text information, the\n",
      "visual information is another significantly important feature in doc-\n",
      "ument representations. Typically, documents contain some visual\n",
      "signals to show the importance and priority of document segments.\n",
      "The visual information can be represented by image features and ef-\n",
      "fectively utilized in document representations. For document-level\n",
      "visual features, the whole image can indicate the document layout,\n",
      "which is an essential feature for document image classification. For\n",
      "word-level visual features, styles such as bold, underline, and italic,\n",
      "are also significant hints for the sequence labeling tasks. There-\n",
      "fore, we believe that combining the image features with traditional\n",
      "text representations can bring richer semantic representations to\n",
      "documents.\n",
      "\n",
      "==>\n",
      "To take advantage of existing pre-trained models and adapt to\n",
      "document image understanding tasks, we use the BERT architecture\n",
      "as the backbone and add two new input embeddings: a 2-D position\n",
      "embedding and an image embedding.\n",
      "\n",
      "==>\n",
      "2-D Position Embedding. Unlike the position embedding that\n",
      "models the word position in a sequence, 2-D position embedding\n",
      "aims to model the relative spatial position in a document. To repre-\n",
      "sent the spatial position of elements in scanned document images,\n",
      "we consider a document page as a coordinate system with the top-\n",
      "left origin. In this setting, the bounding box can be precisely defined\n",
      "by (x0, y0, x1, y1), where (x0, y0) corresponds to the position of the\n",
      "upper left in the bounding box, and (x1, y1) represents the position\n",
      "of the lower right. We add four position embedding layers with two\n",
      "embedding tables, where the embedding layers representing the\n",
      "same dimension share the same embedding table. This means that\n",
      "we look up the position embedding of x0 and x1 in the embedding\n",
      "table X and lookup y0 and y1 in table Y.\n",
      "\n",
      "==>\n",
      "Image Embedding. To utilize the image feature of a document and\n",
      "align the image feature with the text, we add an image embedding\n",
      "layer to represent image features in language representation. In\n",
      "more detail, with the bounding box of each word from OCR results,\n",
      "we split the image into several pieces, and they have a one-to-one\n",
      "correspondence with the words. We generate the image region\n",
      "features with these pieces of images from the Faster R-CNN [19]\n",
      "model as the token image embeddings. For the [CLS] token, we\n",
      "also use the Faster R-CNN model to produce embeddings using the\n",
      "whole scanned document image as the Region of Interest (ROI) to\n",
      "benefit the downstream tasks which need the representation of the\n",
      "[CLS] token.\n",
      "\n",
      "==>\n",
      "Task #1: Masked Visual-Language Model. Inspired by the masked\n",
      "language model, we propose the Masked Visual-language Model\n",
      "(MVLM) to learn the language representation with the clues of 2-D\n",
      "position embeddings and text embeddings. During the pre-training,\n",
      "we randomly mask some of the input tokens but keep the corre-\n",
      "sponding 2-D position embeddings, and then the model is trained\n",
      "to predict the masked tokens given the contexts. In this way, the\n",
      "LayoutLM model not only understands the language contexts but\n",
      "also utilizes the corresponding 2-D position information, thereby\n",
      "bridging the gap between the visual and language modalities.\n",
      "\n",
      "==>\n",
      "Task #2: Multi-label Document Classification. For document im-\n",
      "age understanding, many tasks require the model to generate high-\n",
      "quality document-level representations. As the IIT-CDIP Test Col-\n",
      "lection includes multiple tags for each document image, we also\n",
      "use a Multi-label Document Classification (MDC) loss during the\n",
      "pre-training phase. Given a set of scanned documents, we use the\n",
      "document tags to supervise the pre-training process so that the\n",
      "model can cluster the knowledge from different domains and gener-\n",
      "ate better document-level representation. Since the MDC loss needs\n",
      "the label for each document image that may not exist for larger\n",
      "datasets, it is optional during the pre-training and may not be used\n",
      "for pre-training larger models in the future. We will compare the\n",
      "performance of MVLM and MVLM+MDC in Section 3.\n",
      "\n",
      "==>\n",
      "receipt understanding task as well as a document image classifica-\n",
      "tion task. For the form and receipt understanding tasks, LayoutLM\n",
      "predicts {B, I, E, S, O} tags for each token and uses sequential label-\n",
      "ing to detect each type of entity in the dataset. For the document\n",
      "image classification task, LayoutLM predicts the class labels using\n",
      "the representation of the [CLS] token.\n",
      "\n",
      "==>\n",
      "The performance of pre-trained models is largely determined by\n",
      "the scale and quality of datasets. Therefore, we need a large-scale\n",
      "scanned document image dataset to pre-train the LayoutLM model.\n",
      "Our model is pre-trained on the IIT-CDIP Test Collection 1.0, which\n",
      "contains more than 6 million documents, with more than 11 million\n",
      "scanned document images. Moreover, each document has its cor-\n",
      "responding text and metadata stored in XML files. The text is the\n",
      "content produced by applying OCR to document images. The meta-\n",
      "data describes the properties of the document, such as the unique\n",
      "identity and document labels. Although the metadata contains er-\n",
      "roneous and inconsistent tags, the scanned document images in\n",
      "this large-scale dataset are perfectly suitable for pre-training our\n",
      "model.\n",
      "\n",
      "==>\n",
      "The FUNSD Dataset. We evaluate our approach on the FUNSD\n",
      "dataset for form understanding in noisy scanned documents. This\n",
      "dataset includes 199 real, fully annotated, scanned forms with 9,707\n",
      "semantic entities and 31,485 words. These forms are organized as a\n",
      "list of semantic entities that are interlinked. Each semantic entity\n",
      "comprises a unique identifier, a label (i.e., question, answer, header,\n",
      "or other), a bounding box, a list of links with other entities, and a\n",
      "list of words. The dataset is split into 149 training samples and 50\n",
      "testing samples. We adopt the word-level F1 score as the evaluation\n",
      "metric.\n",
      "\n",
      "==>\n",
      "The SROIE Dataset. We also evaluate our model on the SROIE\n",
      "dataset for receipt information extraction (Task 3). The dataset\n",
      "contains 626 receipts for training and 347 receipts for testing. Each\n",
      "receipt is organized as a list of text lines with bounding boxes. Each\n",
      "receipt is labeled with four types of entities which are {company,\n",
      "date, address, total}. The evaluation metric is the exact match of the\n",
      "entity recognition results in the F1 score.\n",
      "\n",
      "==>\n",
      "The RVL-CDIP Dataset. The RVL-CDIP dataset consists of 400,000\n",
      "grayscale images in 16 classes, with 25,000 images per class. There\n",
      "are 320,000 training images, 40,000 validation images, and 40,000\n",
      "test images. The images are resized, so their largest dimension does\n",
      "not exceed 1,000 pixels. The 16 classes include {letter, form, email,\n",
      "handwritten, advertisement, scientific report, scientific publication,\n",
      "specification, file folder, news article, budget, invoice, presentation,\n",
      "questionnaire, resume, memo}. The evaluation metric is the overall\n",
      "classification accuracy.\n",
      "\n",
      "==>\n",
      "To utilize the layout information of each document, we need to\n",
      "obtain the location of each token. However, the pre-training dataset\n",
      "(IIT-CDIP Test Collection) only contains pure texts while missing\n",
      "\n",
      "==>\n",
      "their corresponding bounding boxes. In this case, we re-process the\n",
      "scanned document images to obtain the necessary layout informa-\n",
      "tion. Like the original pre-processing in IIT-CDIP Test Collection,\n",
      "we similarly process the dataset by applying OCR to document\n",
      "images. The difference is that we obtain both the recognized words\n",
      "and their corresponding locations in the document image. Thanks\n",
      "to Tesseract6, an open-source OCR engine, we can easily obtain the\n",
      "recognition as well as the 2-D positions. We store the OCR results in\n",
      "hOCR format, a standard specification format which clearly defines\n",
      "the OCR results of one single document image using a hierarchical\n",
      "representation.\n",
      "\n",
      "==>\n",
      "We initialize the weight of LayoutLM model with the pre-trained\n",
      "BERT base model. Specifically, our BASE model has the same ar-\n",
      "chitecture: a 12-layer Transformer with 768 hidden sizes, and 12\n",
      "attention heads, which contains about 113M parameters. Therefore,\n",
      "we use the BERT base model to initialize all modules in our model\n",
      "except the 2-D position embedding layer. For the LARGE setting,\n",
      "our model has a 24-layer Transformer with 1,024 hidden sizes and\n",
      "16 attention heads, which is initialized by the pre-trained BERT\n",
      "LARGE model and contains about 343M parameters. Following [4],\n",
      "we select 15% of the input tokens for prediction. We replace these\n",
      "masked tokens with the [MASK] token 80% of the time, a random to-\n",
      "ken 10% of the time, and an unchanged token 10% of the time. Then,\n",
      "the model predicts the corresponding token with the cross-entropy\n",
      "loss.\n",
      "In addition, we also add the 2-D position embedding layers with\n",
      "four embedding representations (x0, y0, x1, y1), where (x0, y0) cor-\n",
      "responds to the position of the upper left in the bounding box, and\n",
      "(x1, y1) represents the position of the lower right. Considering that\n",
      "the document layout may vary in different page size, we scale the\n",
      "actual coordinate to a “virtual” coordinate: the actual coordinate is\n",
      "scaled to have a value from 0 to 1,000. Furthermore, we also use the\n",
      "ResNet-101 model as the backbone network in the Faster R-CNN\n",
      "model, which is pre-trained on the Visual Genome dataset [12].\n",
      "We train our model on 8 NVIDIA Tesla V100 32GB GPUs with a\n",
      "total batch size of 80. The Adam optimizer is used with an initial\n",
      "learning rate of 5e-5 and a linear decay learning rate schedule. The\n",
      "BASE model takes 80 hours to finish one epoch on 11M documents,\n",
      "while the LARGE model takes nearly 170 hours to finish one epoch.\n",
      "\n",
      "==>\n",
      "We evaluate the LayoutLM model on three document image under-\n",
      "standing tasks: Form Understanding, Receipt Understanding,\n",
      "and Document Image Classification. We follow the typical fine-\n",
      "tuning strategy and update all parameters in an end-to-end way on\n",
      "task-specific datasets.\n",
      "\n",
      "==>\n",
      "Form Understanding. This task requires extracting and structur-\n",
      "ing the textual content of forms. It aims to extract key-value pairs\n",
      "from the scanned form images. In more detail, this task includes\n",
      "two sub-tasks: semantic labeling and semantic linking. Semantic\n",
      "labeling is the task of aggregating words as semantic entities and\n",
      "assigning pre-defined labels to them. Semantic linking is the task\n",
      "\n",
      "==>\n",
      "of predicting the relations between semantic entities. In this work,\n",
      "we focus on the semantic labeling task, while semantic linking\n",
      "is out of the scope. To fine-tune LayoutLM on this task, we treat\n",
      "semantic labeling as a sequence labeling problem. We pass the final\n",
      "representation into a linear layer followed by a softmax layer to\n",
      "predict the label of each token. The model is trained for 100 epochs\n",
      "with a batch size of 16 and a learning rate of 5e-5.\n",
      "\n",
      "==>\n",
      "Receipt Understanding. This task requires filling several pre-\n",
      "defined semantic slots according to the scanned receipt images.\n",
      "For instance, given a set of receipts, we need to fill specific slots (\n",
      "i.g., company, address, date, and total). Different from the form un-\n",
      "derstanding task that requires labeling all matched entities and key-\n",
      "value pairs, the number of semantic slots is fixed with pre-defined\n",
      "keys. Therefore, the model only needs to predict the corresponding\n",
      "values using the sequence labeling method.\n",
      "\n",
      "==>\n",
      "Document Image Classification. Given a visually rich document,\n",
      "this task aims to predict the corresponding category for each doc-\n",
      "ument image. Distinct from the existing image-based approaches,\n",
      "our model includes not only image representations but also text and\n",
      "layout information using the multimodal architecture in LayoutLM.\n",
      "Therefore, our model can combine the text, layout, and image in-\n",
      "formation in a more effective way. To fine-tune our model on this\n",
      "task, we concatenate the output from the LayoutLM model and the\n",
      "whole image embedding, followed by a softmax layer for category\n",
      "prediction. We fine-tune the model for 30 epochs with a batch size\n",
      "of 40 and a learning rate of 2e-5.\n",
      "\n",
      "==>\n",
      "Form Understanding. We evaluate the form understanding task\n",
      "on the FUNSD dataset. The experiment results are shown in Table 1.\n",
      "We compare the LayoutLM model with two SOTA pre-trained NLP\n",
      "models: BERT and RoBERTa [16]. The BERT BASE model achieves\n",
      "0.603 and while the LARGE model achieves 0.656 in F1. Compared\n",
      "to BERT, the RoBERTa performs much better on this dataset as it is\n",
      "trained using larger data with more epochs. Due to the time limita-\n",
      "tion, we present 4 settings for LayoutLM, which are 500K document\n",
      "pages with 6 epochs, 1M with 6 epochs, 2M with 6 epochs as well\n",
      "as 11M with 2 epochs. It is observed that the LayoutLM model sub-\n",
      "stantially outperforms existing SOTA pre-training baselines. With\n",
      "the BASE architecture, the LayoutLM model with 11M training\n",
      "data achieves 0.7866 in F1, which is much higher than BERT and\n",
      "RoBERTa with the similar size of parameters. In addition, we also\n",
      "add the MDC loss in the pre-training step and it does bring substan-\n",
      "tial improvements on the FUNSD dataset. Finally, the LayoutLM\n",
      "model achieves the best performance of 0.7927 when using the text,\n",
      "layout, and image information at the same time.\n",
      "In addition, we also evaluate the LayoutLM model with different\n",
      "data and epochs on the FUNSD dataset, which is shown in Table 2.\n",
      "For different data settings, we can see that the overall accuracy\n",
      "is monotonically increased as more epochs are trained during the\n",
      "pre-training step. Furthermore, the accuracy is also improved as\n",
      "more data is fed into the LayoutLM model. As the FUNSD dataset\n",
      "contains only 149 images for fine-tuning, the results confirm that\n",
      "the pre-training of text and layout is effective for scanned document\n",
      "understanding especially with low resource settings.\n",
      "\n",
      "==>\n",
      "LayoutLMBASE (500K, 6 epochs)\n",
      "0.665\n",
      "0.7355\n",
      "0.6985\n",
      "113M\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.6909\n",
      "0.7735\n",
      "0.7299\n",
      "113M\n",
      "LayoutLMBASE (2M, 6 epochs)\n",
      "0.7377\n",
      "0.782\n",
      "0.7592\n",
      "113M\n",
      "LayoutLMBASE (11M, 2 epochs)\n",
      "0.7597\n",
      "0.8155\n",
      "0.7866\n",
      "113M\n",
      "\n",
      "==>\n",
      "Furthermore, we compare different initialization methods for\n",
      "the LayoutLM model including from scratch, BERT and RoBERTa.\n",
      "The results in Table 3 show that the LayoutLMBASE model initial-\n",
      "ized with RoBERTaBASE outperforms BERTBASE by 2.1 points in F1.\n",
      "For the LARGE setting, the LayoutLMLARGE model initialized with\n",
      "RoBERTaLARGE further improve 1.3 points over the BERTLARGE\n",
      "model. We will pre-train more models with RoBERTa as the initial-\n",
      "ization in the future, especially for the LARGE settings.\n",
      "\n",
      "==>\n",
      "Receipt Understanding. We evaluate the receipt understanding\n",
      "task using the SROIE dataset. The results are shown in Table 4. As\n",
      "we only test the performance of the Key Information Extraction\n",
      "task in SROIE, we would like to eliminate the effect of incorrect\n",
      "OCR results. Therefore, we pre-process the training data by using\n",
      "the ground truth OCR and run a set of experiments using the base-\n",
      "line models (BERT & RoBERTa) as well as the LayoutLM model.\n",
      "The results show that the LayoutLMLARGE model trained with 11M\n",
      "\n",
      "==>\n",
      "SCRATCH\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.5630\n",
      "0.6728\n",
      "0.6130\n",
      "BERTBASE\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.6909\n",
      "0.7735\n",
      "0.7299\n",
      "RoBERTaBASE\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.7173\n",
      "0.7888\n",
      "0.7514\n",
      "SCRATCH\n",
      "LayoutLMLARGE (11M, 1 epoch)\n",
      "0.6845\n",
      "0.7804\n",
      "0.7293\n",
      "BERTLARGE\n",
      "LayoutLMLARGE (11M, 1 epoch)\n",
      "0.7536\n",
      "0.8060\n",
      "0.7789\n",
      "RoBERTaLARGE\n",
      "LayoutLMLARGE (11M, 1 epoch)\n",
      "0.7681\n",
      "0.8188\n",
      "0.7926\n",
      "\n",
      "==>\n",
      "document images achieve an F1 score of 0.9524, which is signifi-\n",
      "cantly better than the first place in the competition leaderboard.\n",
      "This result also verifies that the pre-trained LayoutLM not only per-\n",
      "forms well on the in-domain dataset (FUNSD) but also outperforms\n",
      "several strong baselines on the out-of-domain dataset like SROIE.\n",
      "\n",
      "==>\n",
      "Document Image Classification. Finally, we evaluate the docu-\n",
      "ment image classification task using the RVL-CDIP dataset. Doc-\n",
      "ument images are different from other natural images as most of\n",
      "the content in document images are texts in a variety of styles and\n",
      "layouts. Traditionally, image-based classification models with pre-\n",
      "training perform much better than the text-based models, which\n",
      "is shown in Table 5. We can see that either BERT or RoBERTa\n",
      "underperforms the image-based approaches, illustrating that text\n",
      "information is not sufficient for this task, and it still needs layout\n",
      "and image features. We address this issue by using the LayoutLM\n",
      "model for this task. Results show that, even without the image\n",
      "features, LayoutLM still outperforms the single model of the image-\n",
      "based approaches. After integrating the image embeddings, the\n",
      "LayoutLM achieves the accuracy of 94.42%, which is significantly\n",
      "better than several SOTA baselines for document image classifi-\n",
      "cation. It is observed that our model performs best in the \"email\"\n",
      "category while performs worst in the \"form\" category. We will\n",
      "further investigate how to take advantage of both pre-trained Lay-\n",
      "outLM and image models, as well as involve image information in\n",
      "the pre-training step for the LayoutLM model.\n",
      "\n",
      "==>\n",
      "The research of Document Analysis and Recognition (DAR) dates\n",
      "to the early 1990s. The mainstream approaches can be divided\n",
      "into three categories: rule-based approaches, conventional machine\n",
      "learning approaches, and deep learning approaches.\n",
      "\n",
      "==>\n",
      "The rule-based approaches [6, 13, 18, 23] contain two types of anal-\n",
      "ysis methods: bottom-up and top-down. Bottom-up methods [5,\n",
      "13, 23] usually detect the connected components of black pixels as\n",
      "the basic computational units in document images, and the docu-\n",
      "ment segmentation process is to combine them into higher-level\n",
      "structures through different heuristics and label them according\n",
      "to different structural features. Docstrum algorithm [18] is among\n",
      "the earliest successful bottom-up algorithms that are based on the\n",
      "connected component analysis. It groups connected components\n",
      "on a polar structure to derive the final segmentation. [23] use a\n",
      "special distance-metric between different components to construct\n",
      "\n",
      "==>\n",
      "a physical page structure. They further reduced the time complexity\n",
      "by using heuristics and path compression algorithms.\n",
      "The top-down methods often recursively split a page into columns,\n",
      "blocks, text lines, and tokens. [6] propose replacing the basic unit\n",
      "with the black pixels from all the pixels, and the method decom-\n",
      "posed the document using the recursive the X-Y cut algorithm to\n",
      "establish an X-Y tree, which makes complex documents decompose\n",
      "more easily. Although these methods perform well on some doc-\n",
      "uments, they require extensive human efforts to figure out better\n",
      "rules, while sometimes failing to generalize to documents from\n",
      "other sources. Therefore, it is inevitable to leverage machine learn-\n",
      "ing approaches in the DAR research.\n",
      "\n",
      "==>\n",
      "With the development of conventional machine learning, statistical\n",
      "machine learning approaches [17, 22] have become the mainstream\n",
      "for document segmentation tasks during the past decade. [22] con-\n",
      "sider the layout information of a document as a parsing problem,\n",
      "and globally search the optimal parsing tree based on a grammar-\n",
      "based loss function. They utilize a machine learning approach to\n",
      "select features and train all parameters during the parsing process.\n",
      "Meanwhile, artificial neural networks [17] have been extensively\n",
      "applied to document analysis and recognition. Most efforts have\n",
      "been devoted to the recognition of isolated handwritten and printed\n",
      "characters with widely recognized successful results. In addition to\n",
      "the ANN model, SVM and GMM [27] have been used in document\n",
      "layout analysis tasks. For machine learning approaches, they are\n",
      "usually time-consuming to design manually crafted features and\n",
      "difficult to obtain a highly abstract semantic context. In addition,\n",
      "these methods usually relied on visual cues but ignored textual\n",
      "information.\n",
      "\n",
      "==>\n",
      "Recently, deep learning methods have become the mainstream and\n",
      "de facto standard for many machine learning problems. Theoreti-\n",
      "cally, they can fit any arbitrary functions through the stacking of\n",
      "multi-layer neural networks and have been verified to be effective\n",
      "in many research areas. [28] treat the document semantic structure\n",
      "extraction task as a pixel-by-pixel classification problem. They pro-\n",
      "pose a multimodal neural network that considers visual and textual\n",
      "information, while the limitation of this work is that they only\n",
      "used the network to assist heuristic algorithms to classify candidate\n",
      "bounding boxes rather than an end-to-end approach. [26] propose a\n",
      "lightweight model of document layout analysis for mobile and cloud\n",
      "services. The model uses one-dimensional information of images for\n",
      "\n",
      "==>\n",
      "LayoutLMBASE (500K, 6 epochs)\n",
      "0.9388\n",
      "0.9388\n",
      "0.9388\n",
      "113M\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "0.9380\n",
      "0.9380\n",
      "0.9380\n",
      "113M\n",
      "LayoutLMBASE (2M, 6 epochs)\n",
      "0.9431\n",
      "0.9431\n",
      "0.9431\n",
      "113M\n",
      "LayoutLMBASE (11M, 2 epochs)\n",
      "0.9438\n",
      "0.9438\n",
      "0.9438\n",
      "113M\n",
      "\n",
      "==>\n",
      "LayoutLMBASE (500K, 6 epochs)\n",
      "91.25%\n",
      "113M\n",
      "LayoutLMBASE (1M, 6 epochs)\n",
      "91.48%\n",
      "113M\n",
      "LayoutLMBASE (2M, 6 epochs)\n",
      "91.65%\n",
      "113M\n",
      "LayoutLMBASE (11M, 2 epochs)\n",
      "91.78%\n",
      "113M\n",
      "\n",
      "==>\n",
      "VGG-16 [1]\n",
      "90.97%\n",
      "-\n",
      "Stacked CNN Single [2]\n",
      "91.11%\n",
      "-\n",
      "Stacked CNN Ensemble [2]\n",
      "92.21%\n",
      "-\n",
      "InceptionResNetV2 [25]\n",
      "92.63%\n",
      "-\n",
      "LadderNet [20]\n",
      "92.77%\n",
      "-\n",
      "Multimodal Single [3]\n",
      "93.03%\n",
      "-\n",
      "Multimodal Ensemble [3]\n",
      "93.07%\n",
      "-\n",
      "\n",
      "==>\n",
      "inference and compares it with the model using two-dimensional in-\n",
      "formation, achieving comparable accuracy in the experiments. [11]\n",
      "make use of a fully convolutional encoder-decoder network that\n",
      "predicts a segmentation mask and bounding boxes, and the model\n",
      "significantly outperforms approaches based on sequential text or\n",
      "document images. [24] incorporate contextual information into the\n",
      "\n",
      "==>\n",
      "Faster R-CNN model that involves the inherently localized nature\n",
      "of article contents to improve region detection performance.\n",
      "Existing deep learning approaches for DAR usually confront\n",
      "two limitations: (1) The models often rely on limited labeled data\n",
      "while leaving a large amount of unlabeled data unused. (2) Current\n",
      "deep learning models usually leverage pre-trained CV models or\n",
      "\n",
      "==>\n",
      "NLP models, but do not consider the joint pre-training of text and\n",
      "layout. LayoutLM addresses these two limitations and achieves\n",
      "much better performance compared with the previous baselines.\n",
      "\n",
      "==>\n",
      "We present LayoutLM, a simple yet effective pre-training technique\n",
      "with text and layout information in a single framework. Based on\n",
      "the Transformer architecture as the backbone, LayoutLM takes\n",
      "advantage of multimodal inputs, including token embeddings, lay-\n",
      "out embeddings, and image embeddings. Meanwhile, the model\n",
      "can be easily trained in a self-supervised way based on large scale\n",
      "unlabeled scanned document images. We evaluate the LayoutLM\n",
      "model on three tasks: form understanding, receipt understanding,\n",
      "and scanned document image classification. Experiments show\n",
      "that LayoutLM substantially outperforms several SOTA pre-trained\n",
      "models in these tasks.\n",
      "For future research, we will investigate pre-training models with\n",
      "more data and more computation resources. In addition, we will\n",
      "also train LayoutLM using the LARGE architecture with text and\n",
      "layout, as well as involving image embeddings in the pre-training\n",
      "step. Furthermore, we will explore new network architectures and\n",
      "other self-supervised training objectives that may further unlock\n",
      "the power of LayoutLM.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for block in cleaned_blocks:\n",
    "    print(\"==>\")\n",
    "    print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "nltk.data.path.append(\"/Users/harshit/nltk_data\")\n",
    "\n",
    "# Download the punkt tokenizer if you haven't already\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def split_into_paragraphs(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences = [re.sub(\"\\n\", \" \", sentence) for sentence in sentences]\n",
    "    \n",
    "    # Group sentences into paragraphs of 2 sentences each\n",
    "    paragraphs = [' '.join(sentences[i:i+2]) for i in range(0, len(sentences), 2)]\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "sentences = split_into_paragraphs(page_texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "# Define your desired output structure\n",
    "class Datasets(BaseModel):\n",
    "    dataset_names: list[str] = Field(description = \"\"\"Names of datasets mentioned \n",
    "                                     in the text which are used by ML model / algorithm for \n",
    "                                     training.\"\"\")\n",
    "    algorithms: list[str] = Field(description = \"\"\"Names of algorithms mentioned \n",
    "                                     in the text which are used by ML model / algorithm for \n",
    "                                     training / validation \"\"\")\n",
    "\n",
    "# Patch the OpenAI client\n",
    "client1 = instructor.from_openai(OpenAI())\n",
    "\n",
    "text = \"\"\"In order to perform a controlled evaluation, for this\n",
    "experiment we generate preference pairs over generations using a pre-trained sentiment classifier,\n",
    "where p(positive | x, yw) > p(positive | x, yl). For SFT, we fine-tune GPT-2-large until convergence\n",
    "on reviews from the train split of the IMDB dataset (further details in App C.1)\"\"\"\n",
    "\n",
    "# Extract structured data from natural language\n",
    "def get_response1(text):\n",
    "    return client1.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_model=Datasets,\n",
    "        messages=[\n",
    "            {\"role\": \"system\",\n",
    "            \"content\": \"\"\"You're a powerful language model that has been specialized for NER where entities are datasets in the domain of Machine Learning / AI.\n",
    "            Extract the names of datasets mentioned in the given text\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": text}],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshit/Documents/Projects/paper-dots-v2/backend/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "# Define your desired output structure\n",
    "class Datasets(BaseModel):\n",
    "    datasets: list[str] = Field(description = \"\"\"Names of datasets mentioned \n",
    "                                     in the text which are used by ML model / algorithm for \n",
    "                                     training.\"\"\")\n",
    "    methods: list[str] = Field(description = \"\"\"Names of algorithms / methods mentioned \n",
    "                                     in the text\"\"\")\n",
    "\n",
    "# Patch the OpenAI client\n",
    "client2 = instructor.from_gemini(\n",
    "    client=genai.GenerativeModel(\n",
    "        model_name=\"models/gemini-1.5-flash-latest\",\n",
    "    ),\n",
    "    mode=instructor.Mode.GEMINI_JSON,\n",
    ")\n",
    "\n",
    "text = \"\"\"In order to perform a controlled evaluation, for this\n",
    "experiment we generate preference pairs over generations using a pre-trained sentiment classifier,\n",
    "where p(positive | x, yw) > p(positive | x, yl). For SFT, we fine-tune GPT-2-large until convergence\n",
    "on reviews from the train split of the IMDB dataset (further details in App C.1)\"\"\"\n",
    "\n",
    "# Extract structured data from natural language\n",
    "def get_response2(text):\n",
    "    return client2.messages.create(\n",
    "    messages=[\n",
    "            {\"role\": \"system\",\n",
    "            \"content\": \"\"\"You're a powerful language model that has been specialized for Named Entity Recognition.\n",
    "            The possible entities are (1) dataset (2) algorithms / methods mentioned in the text.\n",
    "            Extract the names of datasets and algorithms mentioned in the given text\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": text}],\n",
    "    response_model=Datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-1.5-flash-latest',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client2.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets=[] methods=['Convolutional Neural Networks (CNN)', 'Faster R-CNN', 'Mask R-CNN', 'Graph Convolutional Networks (GCN)']\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    responses = list(executor.map(get_response2, cleaned_blocks[3:4]))\n",
    "    \n",
    "for res in responses:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document AI, or Document Intelligence1, is a relatively new re-\n",
      "search topic that refers techniques for automatically reading, under-\n",
      "standing, and analyzing business documents. Business documents\n",
      "are files that provide details related to a company’s internal and\n",
      "external transactions, which are shown in Figure 1. They may be\n",
      "digital-born, occurring as electronic files, or they may be in scanned\n",
      "form that comes from written or printed on paper. Some common\n",
      "examples of business documents include purchase orders, financial\n",
      "reports, business emails, sales agreements, vendor contracts, letters,\n",
      "invoices, receipts, resumes, and many others. Business documents\n",
      "are critical to a company’s efficiency and productivity. The exact\n",
      "format of a business document may vary, but the information is\n",
      "usually presented in natural language and can be organized in a\n",
      "variety of ways from plain text, multi-column layouts, and a wide\n",
      "variety of tables/forms/figures. Understanding business documents\n",
      "is a very challenging task due to the diversity of layouts and formats,\n",
      "poor quality of scanned document images as well as the complexity\n",
      "of template structures.\n",
      "Nowadays, many companies extract data from business docu-\n",
      "ments through manual efforts that are time-consuming and expen-\n",
      "sive, meanwhile requiring manual customization or configuration.\n",
      "Rules and workflows for each type of document often need to be\n",
      "hard-coded and updated with changes to the specific format or\n",
      "when dealing with multiple formats. To address these problems,\n",
      "document AI models and algorithms are designed to automatically\n",
      "classify, extract, and structuralize information from business doc-\n",
      "uments, accelerating automated document processing workflows.\n",
      "Contemporary approaches for document AI are usually built upon\n",
      "deep neural networks from a computer vision perspective or a natu-\n",
      "ral language processing perspective, or a combination of them. Early\n",
      "attempts usually focused on detecting and analyzing certain parts\n",
      "of a document, such as tabular areas. [7] were the first to propose a\n",
      "table detection method for PDF documents based on Convolutional\n",
      "Neural Networks (CNN). After that, [21, 24, 29] also leveraged more\n",
      "advanced Faster R-CNN model [19] or Mask R-CNN model [9] to\n",
      "further improve the accuracy of document layout analysis. In addi-\n",
      "tion, [28] presented an end-to-end, multimodal, fully convolutional\n",
      "network for extracting semantic structures from document images,\n",
      "taking advantage of text embeddings from pre-trained NLP models.\n",
      "More recently, [15] introduced a Graph Convolutional Networks\n",
      "(GCN) based model to combine textual and visual information for\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_blocks[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Datasets(dataset_names=['LayoutLM', 'IIT-CDIP Test Collection 1.02'])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.from_openai(OpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=client.chat.completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extract the names of datasets and algorithms mentioned in the given text.\\n            The possible entities are (1) dataset (2) algorithms / methods mentioned in the text.\\n            \\n            ## Text:\\n            asa\\n            '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Extract the names of datasets and algorithms mentioned in the given text.\n",
    "            The possible entities are (1) dataset (2) algorithms / methods mentioned in the text.\n",
    "            \n",
    "            ## Text:\n",
    "            {}\n",
    "            \"\"\".format(\"asa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
